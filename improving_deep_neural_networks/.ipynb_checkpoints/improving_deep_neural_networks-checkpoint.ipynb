{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev and Test Sets\n",
    "\n",
    "Finding the best solution for a machine learning task involves several rounds of iterations of various hyperparameters (as well as the usually iterative parameter optimization).\n",
    "\n",
    "To achieve this usually split the data available into train, dev and test sets. This is so that parameters, hyperparameters and expected performance metrics can be estimated in a fair and unbiased way. \n",
    "\n",
    "The traditional splits have been 70% training, 30% dev split. (In the past it wasn't common to have test split and the unbiased estimator of performance wasn't well known as good practice.)\n",
    "\n",
    "Now with big data, depending on the application we might only need 10,000 examples each in dev and test sets. Therefore these days it is common to see splits such as:\n",
    "\n",
    "train: 99%\n",
    "dev: 0.5%\n",
    "test: 0.5%\n",
    "\n",
    "Depending on the application, as long as the dev and test set have a minimum number of examples for hyperparameter seach and an fair estimate of generalization performance, it is possible to push down the proportion of data splits for each.\n",
    "\n",
    "Usually, 0.5% for each of dev and test is seen as acceptable for big data problems.\n",
    "\n",
    "## Bias vs Variance\n",
    "\n",
    "For two dimensional features we can get a good understanding of bias and variance. High bias measures underfitting while high variance measures overfitting.\n",
    "\n",
    "Once we go beyond 3 dimensions it's difficult to get a sense of bias and variance using diagrams.\n",
    "\n",
    "![](high_bias_high_variance.jpeg)\n",
    "\n",
    "For more features, using dev and test set splits is a good measure to understand bias and variance. Another measure that helps a lot is the Bayes error (which is often proxied by the human level error - at least for unstructured data).\n",
    "\n",
    "\n",
    "| Metric | High Bias | Low Bias | High Variance | Low Variance | Low Bias and Low Variance | High Bias and High Variance |\n",
    "|--|--|--|--|--|--|--|\n",
    "| Bayes (Almost Human) |0.5%|0.5%|0.5%|0.5%|0.5%|0.5%|\n",
    "| Train |10%|0.5%|1%|0.5%|0.5%|10%| \n",
    "| Dev |11%|0.8%|10%|3%|0.8%|20%| \n",
    "| Test |11%|1%|11%|3.5%|0.9%|23%| \n",
    "\n",
    "\n",
    "\n",
    "## Basic Recipe for Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "## Regularization\n",
    "\n",
    "## Why Regularization Reduces Overfitting\n",
    "\n",
    "## Dropout Regularization\n",
    "\n",
    "## Understanding Dropout\n",
    "\n",
    "## Other Regularization Methods\n",
    "\n",
    "## Normalizing Inputs\n",
    "\n",
    "## Vanishing or Exploding Gradients\n",
    "\n",
    "## Weight Initialization in a Deep Network\n",
    "\n",
    "## Numerical Approximations of Gradients\n",
    "\n",
    "## Gradient Checking\n",
    "\n",
    "## Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
