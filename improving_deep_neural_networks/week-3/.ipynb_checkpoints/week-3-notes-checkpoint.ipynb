{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Notes\n",
    "\n",
    "## Tuning Process\n",
    "\n",
    "Hyperparameters are parts of the neural network learning architecture that are treated as fixed. They are not learned from the data directly - because they would lower the training error but not aid generalization. Additionally, usually there is no clean way to learn them from data even if we wanted - because of the computational load.\n",
    "\n",
    "Instead we rely on another split of the data and use a pseudo empirical Bayes procedure to find the best hyperparameters. This is a great discussion from [stackoverflow](https://stats.stackexchange.com/questions/365762/why-dont-we-just-learn-the-hyper-parameters).\n",
    "\n",
    "```\n",
    "A hyperparameter typically corresponds to a setting of the learning algorithm, rather than one of its parameters. In the context of deep learning, for example, this is exemplified by the difference between something like the number of neurons in a particular layer (a hyperparameter) and the weight of a particular edge (a regular, learnable parameter).\n",
    "\n",
    "Why is there a difference in the first place? The typical case for making a parameter a hyperparameter is that it is just not appropriate to learn that parameter from the training set. For example, since it's always easier to lower the training error by adding more neurons, making the number of neurons in a layer a regular parameter would always encourage very large networks, which is something we know for a fact is not always desirable (because of overfitting).\n",
    "\n",
    "To your question, it's not that we don't learn the hyper-parameters at all. Setting aside the computational challenges for a minute, it's very much possible to learn good values for the hyperparameters, and there are even cases where this is imperative for good performance; all the discussion in the first paragraph suggests is that by definition, you can't use the same data for this task.\n",
    "\n",
    "Using another split of the data (thus creating three disjoint parts: the training set, the validation set, and the test set, what you could do in theory is the following nested-optimization procedure: in the outer-loop, you try to find the values for the hyperparameters that minimize the validation loss; and in the inner-loop, you try to find the values for the regular parameters that minimize the training loss.\n",
    "\n",
    "This is possible in theory, but very expensive computationally: every step of the outer loop requires solving (till completion, or somewhere close to that) the inner-loop, which is typically computationally-heavy. What further complicates things is that the outer-problem is not easy: for one, the search space is very big.\n",
    "\n",
    "There are many approaches to overcome this by simplifying the setup above (grid search, random search or model-based hyper-parameter optimization), but explaining these is well beyond the scope of your question. As the article you've referenced also demonstrates, the fact that this is a costly procedure often means that researchers simply skip it altogether, or try very few setting manually, eventually settling on the best one (again, according to the validation set). To your original question though, I argue that - while very simplistic and contrived - this is still a form of \"learning\".\n",
    "\n",
    "```\n",
    "\n",
    "### Examples of Hyperparameters\n",
    "\n",
    "THere are many kinds of hyperparameters that one might want to tune. A good strategy (found empirically) is to tune in the following sequence:\n",
    "\n",
    "1. learning rate $alpha$\n",
    "\n",
    "1. number of hidden units, minibatch size, $\\beta$ of Adam algorithm\n",
    "\n",
    "1. number of layers, learing rate decay\n",
    "\n",
    "1. Almost never done in practice, but is possible to tune $\\beta_1, \\beta_2, \\epsilon$. Defaults (0.9, 0.999, $10^{-8}$) are usually good enough.\n",
    "\n",
    "\n",
    "### Hyperparameter Search Strategy\n",
    "\n",
    "There are some strategies that can be tried:\n",
    "\n",
    "\n",
    "1. grid search - usually discouraged as computationally expensive.\n",
    "\n",
    "1. random search - known to produce good results, but can be wasteful. Can also use coarse to fine search scheme where after some time, finding a promising neighborhood with good possibilities, we focus our search around that area - this is similar to the next section.\n",
    "\n",
    "1. Bayesian hyperparameter optimization - balancing exploitation vs exploration.\n",
    "\n",
    "There has been some innovation in this space, notes can be found [here](https://www.automl.org/wp-content/uploads/2018/09/chapter1-hpo.pdf)\n",
    "\n",
    "## Using An Appropriate Scale\n",
    "\n",
    "If we have a range for a parameter - which in itself is a task, then often using an appropriate scale that reflects the relative change can be a better sampling space. One good choice can be to sample from the log space. \n",
    "\n",
    "This could be for quantities such as:\n",
    "\n",
    "- $\\alpha$ sampled on log space\n",
    "- $\\beta$ sampled from $log(1-\\beta)$\n",
    "\n",
    "\n",
    "## HyperParameter Tuning In Practice\n",
    "\n",
    "There are two ways to do hyperparameter search in practice.\n",
    "\n",
    "### Panda Approach: Not much compute or data\n",
    "\n",
    "The first is to babysit and mange a single model, adjusting hyperparameters manually and checking updates of performance. This is out of vogue in the era of cheap compute and big data. It is akin to a panda that has one offspring and takes care of it.\n",
    "\n",
    "### Caviar Approach: A lot of compute and data\n",
    "\n",
    "The second approach is much more used these days. This is to try many hundreds/thousands of hyperparameter settings and choose that which performs best. Such an approach is like caviar, with fish laying thousands of eggs and only some surviving with little supervision.\n",
    "\n",
    "## Normalizing Activations In A Network\n",
    "\n",
    "### Normalizing Inputs\n",
    "\n",
    "We have seen that normalizing inputs (layer 0 activations) is a good idea because it makes the surface easier to navigate on a similar scale.  The same argument is used to justify batch norm, that is normalizing the linear combinations in each layer. This has been shown empirically to improve training and reduce problems like exploding and vanishing gradients.\n",
    "\n",
    "Some papers suggest normalizing activations, but in practice most systems are made by normalizing linear combinations across the layers. The equations are as below:\n",
    "\n",
    "For a fixed layer [l] and a minibatch t of size $q$, we have linear combinations for each of the q examples in this minibatch.\n",
    "\n",
    "$z^{[l] \\{t\\}, (1)}, \\ldots, z^{[l] \\{t\\}, (q)} $\n",
    "\n",
    "\n",
    "Note that each of these z's is a vector with `z.shape` $= (n^[l],1)$\n",
    "\n",
    "We now normalize these $z$'s assuming statistical independence and get location and scale vector parameters $\\mu$ and $\\sigma$. These have the same shape as the z's, namely $(n^[l],1)$\n",
    "\n",
    "### Normalizing Hidden Layers Linear Combinations\n",
    "\n",
    "$\\mu$ and $\\sigma$ are vectors (of shape $(n^{[l]},1)$ same as the z's) of the $i^{th}$ example on the $l^{th}$ layer of the $t^{th}$ minibatch estimated as follows.\n",
    "\n",
    "\n",
    "$\\mu^{[l], \\{t\\} } = \\frac{1}{m} \\sum_{i=1}^{i=q}  (z^{[l],\\{t\\}, (i)})$\n",
    "\n",
    "$ (\\sigma^{[l], \\{t\\}})^2 = \\frac{1}{m} \\sum_{i=1}^{i=q}  (z^{ [l],\\{t\\}, (i)} - \\mu)^2$\n",
    "\n",
    "The assumption is that there is not cross correlation.\n",
    "\n",
    "Then the normalized values of z's are given as subtracting the mean estimate and dividing by the standard devation:\n",
    "\n",
    "$z^{[l],\\{t\\}, (i)}_{\\textbf{norm}} = \\frac{z^{[l],\\{t\\}, (i)} - \\mu^{[l], \\{t\\} }}{\\sigma^{[l], \\{t\\}}}$\n",
    "\n",
    "\n",
    "### Rescaling Normalized Linear Combinations\n",
    "\n",
    "As will be explained later, it is useful to scale the normalized values to have arbitrary mean and variance. The reason is that certain centering and scalings are better for certain activation functions. For example, being scaling and shifting the tanh function results in the sigmoid function.\n",
    "\n",
    "For a given location $\\beta$ and scale $\\gamma$ (which we will see later can be learned from the training set using gradient descent), we have:\n",
    "\n",
    "$\\widetilde{z}^{[l],\\{t\\}, (i)} = \\gamma^{[l]} z^{[l],\\{t\\}, (i)}_{\\textbf{norm}} + \\beta^{[l]}$\n",
    "\n",
    "These $\\widetilde{z}$'s are used as arguments to the activation functions. The $\\gamma$ and $\\beta$ parameters need to be estimated, but this can usually be done with a single command added to most neural network frameworks such as tensorflow or pytorch.\n",
    "\n",
    "### Why Rescale Linear Combinations?\n",
    "\n",
    "Why do we do this? More details will be provided later, but broadly:\n",
    "\n",
    "- Allows centering and scaling output to activation functions\n",
    "\n",
    "- Consuming neurons can rely on some stability of the range and likely input neurons\n",
    "\n",
    "- Better convergence properties\n",
    "\n",
    "- Some small regularization effect\n",
    "\n",
    "## Fitting Batch Norm Into Networks\n",
    "\n",
    "In order to fit batch norm and learn the parameters needed, we need to make slight modifications. In particular the bias term falls away and is replaced by the $\\beta^{[l]}$ term.\n",
    "\n",
    "$\\beta$ and $\\gamma$ need to be learned using gradient descent, along with the $W$ parameters. That means initial conditions and then forward and backward propagation to get updates for the iterations of the gradient descent algorithm.\n",
    "\n",
    "Due to the normalizing applied to z's, the bias parameters disappear and do not need to be estimated.\n",
    "\n",
    "\n",
    "## Why Does Batch Norm Work?\n",
    "\n",
    "BatchNorm works because it introduces a tranformation that makes covariate shifts invariant to the learned network. Usually a shifting of covariates in the hidden layers causes problems. However if they scale and location are estimated parameters from a normalized linear combination, then the algorithm can learn effectively.\n",
    "\n",
    "\n",
    "- Allows centering and scaling output to activation functions\n",
    "\n",
    "- Consuming neurons can rely on some stability of the range and likely input neurons\n",
    "\n",
    "- Better convergence properties\n",
    "\n",
    "- Some small regularization effect due to mini batch specific mean and variance estimates - which introduces some noise.\n",
    "\n",
    "## Batch Norm At Test Time\n",
    "\n",
    "\n",
    "At test time, estimating the normalization parameters is done by taking exponentially weighted average of the mean and variance parameters. This can be used to normalize the test cases and the apply the transformations to get out predictions.\n",
    "\n",
    "Ideally we would estimate the mean and variance using all the data, but that would be a computationally expensive pass over all the data. Instead, by calculating an exponentially weighted moving average, we have a good estimate for little cost.\n",
    "\n",
    "In practice, most of the frameworks do this or something similar in the background, so it doesn't need to be coded from scratch.\n",
    "\n",
    "\n",
    "## SoftMax Regression\n",
    "\n",
    "In softmax regression (also called multinomial regression) the output is one (and only one) of several predefined classes, say K of them. Multinomial regression is a linear classifier, the multiclass extension of logistic regression. \n",
    "\n",
    "The final layer activation function is defined as taking in the final layer linear combinations and working on all the components of this layer to produce a vector of the same length that can be seen as the probability of each class. That is it takes K linear combinations (a K length vector) and returns K probabilities, one for each class. The actual transformation function is from the $Z^[1]$ vector of the final layer to the final output estimate vector:\n",
    "\n",
    "$ \\Pr( Y = C_0 | x)  = \\frac{exp(Z_i)}{\\sum_i exp(Z_i)} $\n",
    "\n",
    "$ \\Pr( Y = C_1 | x)  = \\frac{exp(Z_i)}{\\sum_i exp(Z_i)} $\n",
    "\n",
    "$ \\Pr( Y = C_2 | x)  = \\frac{exp(Z_i)}{\\sum_i exp(Z_i)} $\n",
    "\n",
    "$ \\vdots $\n",
    "\n",
    "$ \\Pr( Y = C_{K-1} | x)  = \\frac{exp(Z_i)}{\\sum_i exp(Z_i)}$\n",
    "\n",
    "![](softmax_regression.gif)\n",
    "\n",
    "### Why The Name SoftMax?\n",
    "\n",
    "The name SoftMax is used because assignment of the class is usually based the maximum of the soft distribution output. Hard max would be when the output is a vector with only one non-zero component and that entry must be 1.\n",
    "\n",
    "### How Is This Related To Logistic Regression?\n",
    "\n",
    "If we have $K=2$ and realize that for this case:\n",
    "\n",
    "$ \\Pr( Y = C_1 | x) = 1 - \\Pr( Y = C_0 | x)$\n",
    "\n",
    "Then as we will see, the cost function reduces to the logistic classifier cost function when there are no hidden layers.\n",
    "\n",
    "## Training SoftMax on Neural Networks\n",
    "\n",
    "SoftMax can be used as the activation function of the final output layer of a neural network that is classifying one of K classes. We look at the loss function to understand how this can be setup.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "If $y$ is a vector of the actual labels and $\\hat{y}$ is the estimate given features $x$, the mapping learned from historical data examples, then the canonical loss function is: \n",
    "\n",
    "For vectors y and $\\hat{y}$ of length $K$\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y})  = -\\sum_{j=0}^{j=K-1} y_j log(\\hat{y_j})\n",
    "$$\n",
    "\n",
    "This is the NLL and Cross Entropy Loss.\n",
    "\n",
    "### BackPropagation \n",
    "\n",
    "In the backpropagation algorithm, for the final layer, we have: \n",
    "\n",
    "$dZ^{[L]} = y - \\hat{y}$\n",
    "\n",
    "and the chain rule ensures that backprop follows as before.\n",
    "\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "TensorFlow (`tf`) is a Python language framework for deep learning and matrix based programming open sourced by Google. It is based off an earlier project that Google used internally called DistBelief.\n",
    "\n",
    "Tensorflow has many API layers (for web, servers, cellphones, execution modes, etc.). This can make it seem complicated. It also moved from a DAG model that needed to be compiled to a more pythonic eager execution mode. This is in addition to integration with the keras easy use API mode.\n",
    "\n",
    "One of the major benefits of `tf` is that once the forward prop is specified (usually in the `tf` idiomatic way), it will automatically calculate the backprop using rules for simple operations and expressions.\n",
    "\n",
    "Below is a simple example of minimizing a one variable convex function using gradient descent variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize Convex Quadratic Function\n",
    "\n",
    "Suppose we have the function:\n",
    "\n",
    "$J(w) = w^2 -10w +25$\n",
    "\n",
    "We want to find the point w where it is minimized. This is easy to do algebraically. We can factor the square \n",
    "\n",
    "$J(w) = (w-5)^2$\n",
    "\n",
    "and it's clear that the function is minimized when $w = 5$.\n",
    "\n",
    "However, just to get a feel for the technology with a simple example, we will do it with gradient descent using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# define variable that will change \n",
    "w = tf.Variable(0.0, dtype=tf.float32)\n",
    "# define function J using tf API commands\n",
    "J = tf.add(tf.add(tf.square(w),tf.multiply(-10.0,w)),25.0)\n",
    "# set learning rate hyperparameter - no tuning\n",
    "learning_rate = 0.01\n",
    "# set optimizer with learning rate, function and minimize requirement\n",
    "# this creates the function as good to run, but is not actually run\n",
    "# note there is no need to explicitly set gradient function\n",
    "#train = tf.train.GradientDescentOptimizer(learning_rate).minimize(J)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(J)\n",
    "\n",
    "#### Idiomatic tf Session compiling the model ###\n",
    "# initialize parameters\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "## see impact of one gradient descent update iteration to the parameters\n",
    "session.run(train)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run gradient descent for 1000 iterations and the one done before\n",
    "for i in range(1000):\n",
    "    session.run(train)\n",
    "\n",
    "session.run(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small constant learning rate, we see that the gradient descent optimizer converges very close to the true optima. In fact if we switch out the code above with one line to specify the Adam optimizer the convergence is in 1 iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tf.session doing?\n",
    "\n",
    "In the background, `tf.session` are moving the model into very efficient code that is optimized to make use of the cache of the CPU. In fact it can even move the data and model to a GPU or specialized chips. This is the DAG part. \n",
    "\n",
    "When `tf.run` is called, the model is executed on the chips and results transported back to the main python thread. As there are resources being managed and handled at tf.session and tf.run, making use of the with keyword is important. This will take care of error handling and managing resources.\n",
    "\n",
    "``` python\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(session.run(w))\n",
    "    session.run(train)\n",
    "```\n",
    "\n",
    "\n",
    "Again remember, there are dozens of APIs to `tf` and so there are many ways to do this. The most popular (for model development) these days being to use keras and eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For a second more machine learning specific example, we look at multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here a more complex example of the hello world dataset for neural networks - MNIST. The training of model parameters is done via the Adam optimizer on sparse categorical loss. Notice how this uses the DAG computation graph which needs to be compiled and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 11s 1us/step\n",
      "WARNING:tensorflow:From /Users/ravikalia/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ravikalia/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.2199 - acc: 0.9340\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0956 - acc: 0.9707\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0683 - acc: 0.9790\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.0529 - acc: 0.9826\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0417 - acc: 0.9867\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0622 - acc: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.062195106509223115, 0.9802]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how in all cases we didn't need to specify the gradient, it was done in the background by the `tf` symbolic engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
