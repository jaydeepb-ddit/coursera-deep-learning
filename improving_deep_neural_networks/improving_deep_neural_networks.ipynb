{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev and Test Sets\n",
    "\n",
    "Finding the best solution for a machine learning task involves several rounds of iterations of various hyperparameters (as well as the usually iterative parameter optimization).\n",
    "\n",
    "To achieve this usually split the data available into train, dev and test sets. This is so that parameters, hyperparameters and expected performance metrics can be estimated in a fair and unbiased way. \n",
    "\n",
    "The traditional splits have been 70% training, 30% dev split. (In the past it wasn't common to have test split and the unbiased estimator of performance wasn't well known as good practice.)\n",
    "\n",
    "Now with big data, depending on the application we might only need 10,000 examples each in dev and test sets. Therefore these days it is common to see splits such as:\n",
    "\n",
    "train: 99%\n",
    "dev: 0.5%\n",
    "test: 0.5%\n",
    "\n",
    "Depending on the application, as long as the dev and test set have a minimum number of examples for hyperparameter seach and an fair estimate of generalization performance, it is possible to push down the proportion of data splits for each.\n",
    "\n",
    "Usually, 0.5% for each of dev and test is seen as acceptable for big data problems.\n",
    "\n",
    "## Bias vs Variance\n",
    "\n",
    "For two dimensional features we can get a good understanding of bias and variance. High bias measures underfitting while high variance measures overfitting.\n",
    "\n",
    "Once we go beyond 3 dimensions it's difficult to get a sense of bias and variance using diagrams.\n",
    "\n",
    "![](high_bias_high_variance.jpeg)\n",
    "\n",
    "For more features, using dev and test set splits is a good measure to understand bias and variance. Another measure that helps a lot is the Bayes error (which is often proxied by the human level error - at least for unstructured data).\n",
    "\n",
    "\n",
    "| Metric | High Bias | Low Bias | High Variance | Low Variance | Low Bias and Low Variance | High Bias and High Variance |\n",
    "|--|--|--|--|--|--|--|\n",
    "| Bayes (Almost Human) |0.5%|0.5%|0.5%|0.5%|0.5%|0.5%|\n",
    "| Train |10%|0.5%|1%|0.5%|0.5%|10%| \n",
    "| Dev |11%|0.8%|10%|3%|0.8%|20%| \n",
    "| Test |11%|1%|11%|3.5%|0.9%|23%| \n",
    "\n",
    "## Basic Recipe for Machine Learning\n",
    "\n",
    "There is a systematic way to remedy problems such as high bias and high variance. There is a lot more to this topic, but at the basic level we suggest the following:\n",
    "\n",
    "- High Bias (Underfitting)\n",
    "\n",
    "    For models that suffer high bias, the following can be tried:\n",
    "    \n",
    "    - increase model capacity/a bigger network/ more parameters\n",
    "\n",
    "    - introduce more features/synthesize more features\n",
    "\n",
    "    - train for longer (more iterations, smaller convergence criteria) if using an iterative algorithm\n",
    "\n",
    "    - train with a different optimizer (RMSProp, Adam, Ada)\n",
    "\n",
    "    - train from different initial conditions\n",
    "\n",
    "    - search extensively for better neural network architecture/hyperparameters    \n",
    "\n",
    "- High Variance (Overfitting)\n",
    "\n",
    "    For models that suffer from high variance, try:\n",
    "\n",
    "    - decrease model capacity/a smaller network/less parameters\n",
    "\n",
    "    - reduce number of features (for example PCA, AIC)\n",
    "\n",
    "    - regularize parameters (penalty on size of parameters - L1, L2 or dropout)\n",
    "\n",
    "    - use more training data to reduce variability in parameter estimates\n",
    "\n",
    "    - average bootstrapped models (Bagging)\n",
    "\n",
    "    - if using classifier check if data is fairly balanced. If not use a balancing strategey (over/undersampling, SMOTE)\n",
    "\n",
    "    - search extensively for better neural network architecture/hyperparameters\n",
    "\n",
    "### Bias Variance Tradeoff\n",
    "\n",
    "Before deep learning models and the big data era, there used to be a lot of talk of trading off bias against variance. One had to accept either higher variance or higher bias, trying to find the best balance for the task at hand.\n",
    "\n",
    "However now with more data, more compute and perhaps more understanding in the ML community of good practice we just need to change some hyperparameters, tweak the algorithm or collect more data and that can help get down to the Bayes error rate for a problem.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Regularization is used to remedy overfitting (high variance) problems. This is because it is often cheaper than the effort needed to collect and process more data - the other default remedy for overfitting.\n",
    "\n",
    "There are several kinds of regularization. Thr most popular ones these days are:\n",
    "\n",
    "- parameter regularization - L1 (Lasso) or L2 (Ridge)\n",
    "- dropout \n",
    "- data augmentation \n",
    "- early stopping: not preferred as it is not an orthogonal technique; it impacts the optimization of the cost function and also regularization at the same time.\n",
    "\n",
    "Below we start with parameter regularization.\n",
    "\n",
    "### Parameter Regularization\n",
    "\n",
    "One way to remedy high variance is reduce the number of parameters or model capacity. In this vein we could also try to make parameters very small or zero by placing a penalty against large parameter values. We do this so as to make the fitting algorithm prefer models with smaller or fewer parameters.\n",
    "\n",
    "From one perspective this is the same as having a Bayesian prior that has an expectation that model parameters will be small or fewer (many model parameters that are 0).\n",
    "\n",
    "L2 regularization adds a penalty to the cost function which prefers models to have smaller (in absolute sense) parameter values. L1 adds a penalty to the cost function which prefers models to have fewer parameters (by preferring parameters to be 0).\n",
    "\n",
    "In both cases, if there is strong data (many occurrences/large correlations) for parameter values to be non-zero/non-small then this will still come through in the final optimized parameters selected.\n",
    "\n",
    "A controlling hyper parameter $\\lambda$ determines the amount of regularization - it chooses the amount of smoothing to be applied. There is a sensible range of values for which hyperparameter tuning can be applied.\n",
    "\n",
    "#### L2 Regularization\n",
    "\n",
    "L2 regularization augments the cost function with the averaged Frobenius (L2) norm of the parameters of the model in each layer, over all layers. That is to say, if the cost function (loss function averaged over all examples) is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}, \\mathbf{B}) = \\frac{1}{m}\\sum_i L(y^{(i)}, \\hat{y(\\mathbf{W}, \\mathbf{B})}^{(i)})\n",
    "$$\n",
    "\n",
    "Then the cost function for a regularized model, or regularized cost function is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}, \\mathbf{B}) =  \\frac{1}{m} \\sum_i L(y^{(i)}, \\hat{y(\\mathbf{W}, \\mathbf{B})})^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L}|| W^{[l]} ||_{F}^{2}\n",
    "$$\n",
    "\n",
    "Where $|| W^{[l]} ||_{F}^{2} = \\sum_i \\sum_j (w_{i,j}^{[l]})^2$\n",
    "\n",
    "This regularized cost function prefers solutions that have smaller (in the absolute value sense) parameters, there can be many parameters, but they will tend to be smaller.\n",
    "\n",
    "Note: Technically the biases $B^{[l]}$ should be added to the cost function - however since they are far fewer than the weight parameters - it is possible to not regularize them and still get good results.\n",
    "\n",
    "#### L1 Regularization\n",
    "\n",
    "L1 Regularization augments the cost function by the L1 metric applied to all weight parameters. This is the averaged of the absolute weight parameters.\n",
    "\n",
    "$$\n",
    "J(\\mathbf{W}, \\mathbf{B}) =  \\frac{1}{m} \\sum_i L(y^{(i)}, \\hat{y(\\mathbf{W}, \\mathbf{B})})^{(i)}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L}| W^{[l]} |\n",
    "$$\n",
    "\n",
    "Where $| W^{[l]} | = \\sum_i \\sum_j| w_{i,j}^{[l]}|$\n",
    "\n",
    "This regularized cost function prefers models which have many weights being zero. It means that sparse models are preferred to more complex ones.\n",
    "\n",
    "### Gradient Descent: L2 Regularization\n",
    "\n",
    "The gradient descent formulas are very similar, only now with a weight decay element which is related to the regularization parameter $\\lambda$.\n",
    "\n",
    "If the original update equations for gradient descent are:\n",
    "\n",
    "$$\n",
    "W = W - \\alpha dW\n",
    "$$\n",
    "\n",
    "And with L2 regularization, they become:\n",
    "\n",
    "$$\n",
    "W = W(1- \\frac{\\alpha \\lambda}{m}) - \\alpha dW\n",
    "$$\n",
    "\n",
    "With some rearrangement, we can see this as a weight decay version the of original equations.\n",
    "\n",
    "In practice it is a lot more common to see L2 regularization than L1 regularization. This might be related to the additional work and computation that is needed to code up and iterate to solve L1 regularization problems.\n",
    "\n",
    "## Why Regularization Reduces Overfitting\n",
    "\n",
    "Regularized cost functions tend to optimize and reduce high variance problems. The reasons for this are subtle, but some intuitions are that:\n",
    "\n",
    "* they prefer functions where the weights are close to 0, because of the penalty increasing the cost function. If weights are close to 0, then the model is simpler, with fewer non-linear interactions.\n",
    "\n",
    "* they make the model more linear by making the linear combination centered around the linear part of the activation function and this propagates up the layers.\n",
    "\n",
    "### Graph Regularized Cost Function\n",
    "\n",
    "When graphing cost functions, we expect the cost to decrease at each iteration of the gradient descent. \n",
    "\n",
    "This is true for regularized versions too, just remember to graph against the regularized cost function not the original cost function. This is because gradient descent now applies to the cost function with penalty and the parameter search is trying to reduce the cost with the penalty included. \n",
    "\n",
    "## Dropout Regularization\n",
    "\n",
    "\n",
    "## Understanding Dropout\n",
    "\n",
    "## Other Regularization Methods\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "## Normalizing Inputs\n",
    "\n",
    "## Vanishing or Exploding Gradients\n",
    "\n",
    "## Weight Initialization in a Deep Network\n",
    "\n",
    "## Numerical Approximations of Gradients\n",
    "\n",
    "## Gradient Checking\n",
    "\n",
    "## Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
