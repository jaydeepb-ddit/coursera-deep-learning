{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Dev and Test Sets\n",
    "\n",
    "Finding the best solution for a machine learning task involves several rounds of iterations of various hyperparameters (as well as the usually iterative parameter optimization).\n",
    "\n",
    "To achieve this usually split the data available into train, dev and test sets. This is so that parameters, hyperparameters and expected performance metrics can be estimated in a fair and unbiased way. \n",
    "\n",
    "The traditional splits have been 70% training, 30% dev split. (In the past it wasn't common to have test split and the unbiased estimator of performance wasn't well known as good practice.)\n",
    "\n",
    "Now with big data, depending on the application we might only need 10,000 examples each in dev and test sets. Therefore these days it is common to see splits such as:\n",
    "\n",
    "train: 99%\n",
    "dev: 0.5%\n",
    "test: 0.5%\n",
    "\n",
    "Depending on the application, as long as the dev and test set have a minimum number of examples for hyperparameter seach and an fair estimate of generalization performance, it is possible to push down the proportion of data splits for each.\n",
    "\n",
    "Usually, 0.5% for each of dev and test is seen as acceptable for big data problems.\n",
    "\n",
    "## Bias vs Variance\n",
    "\n",
    "For two dimensional features we can get a good understanding of bias and variance. High bias measures underfitting while high variance measures overfitting.\n",
    "\n",
    "Once we go beyond 3 dimensions it's difficult to get a sense of bias and variance using diagrams.\n",
    "\n",
    "![](high_bias_high_variance.jpeg)\n",
    "\n",
    "For more features, using dev and test set splits is a good measure to understand bias and variance. Another measure that helps a lot is the Bayes error (which is often proxied by the human level error - at least for unstructured data).\n",
    "\n",
    "\n",
    "| Metric | High Bias | Low Bias | High Variance | Low Variance | Low Bias and Low Variance | High Bias and High Variance |\n",
    "|--|--|--|--|--|--|--|\n",
    "| Bayes (Almost Human) |0.5%|0.5%|0.5%|0.5%|0.5%|0.5%|\n",
    "| Train |10%|0.5%|1%|0.5%|0.5%|10%| \n",
    "| Dev |11%|0.8%|10%|3%|0.8%|20%| \n",
    "| Test |11%|1%|11%|3.5%|0.9%|23%| \n",
    "\n",
    "## Basic Recipe for Machine Learning\n",
    "\n",
    "There is a systematic way to remedy problems such as high bias and high variance. There is a lot more to this topic, but at the basic level we suggest the following:\n",
    "\n",
    "- High Bias (Underfitting)\n",
    "\n",
    "    For models that suffer high bias, the following can be tried:\n",
    "    \n",
    "    - increase model capacity/a bigger network/ more parameters\n",
    "\n",
    "    - introduce more features/synthesize more features\n",
    "\n",
    "    - train for longer (more iterations, smaller convergence criteria) if using an iterative algorithm\n",
    "\n",
    "    - train with a different optimizer (RMSProp, Adam, Ada)\n",
    "\n",
    "    - train from different initial conditions\n",
    "\n",
    "    - search extensively for better neural network architecture/hyperparameters    \n",
    "\n",
    "- High Variance (Overfitting)\n",
    "\n",
    "    For models that suffer from high variance, try:\n",
    "\n",
    "    - decrease model capacity/a smaller network/less parameters\n",
    "\n",
    "    - reduce number of features (for example PCA, AIC)\n",
    "\n",
    "    - regularize parameters (penalty on size of parameters - L1, L2 or dropout)\n",
    "\n",
    "    - use more training data to reduce variability in parameter estimates\n",
    "\n",
    "    - average bootstrapped models (Bagging)\n",
    "\n",
    "    - if using classifier check if data is fairly balanced. If not use a balancing strategey (over/undersampling, SMOTE)\n",
    "\n",
    "    - search extensively for better neural network architecture/hyperparameters\n",
    "\n",
    "### Bias Variance Tradeoff\n",
    "\n",
    "Before deep learning models and the big data era, there used to be a lot of talk of trading off bias against variance. One had to accept either higher variance or higher bias, trying to find the best balance for the task at hand.\n",
    "\n",
    "However now with more data, more compute and perhaps more understanding in the ML community of good practice we just need to change some hyperparameters, tweak the algorithm or collect more data and that can help get down to the Bayes error rate for a problem.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Why Regularization Reduces Overfitting\n",
    "\n",
    "## Dropout Regularization\n",
    "\n",
    "## Understanding Dropout\n",
    "\n",
    "## Other Regularization Methods\n",
    "\n",
    "## Normalizing Inputs\n",
    "\n",
    "## Vanishing or Exploding Gradients\n",
    "\n",
    "## Weight Initialization in a Deep Network\n",
    "\n",
    "## Numerical Approximations of Gradients\n",
    "\n",
    "## Gradient Checking\n",
    "\n",
    "## Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
