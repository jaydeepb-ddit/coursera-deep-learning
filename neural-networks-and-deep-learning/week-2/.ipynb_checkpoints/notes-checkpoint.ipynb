{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2 Notes\n",
    "\n",
    "It's possible to cast a logistic regression as a simple neural network.  This is what we do here, along with introducting the backward pass and forward pass as part of the computation graph. Finally, we introduce a vectorized implementation and demonstrate its improved efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as a Neural Network\n",
    "Casting logistic regression as a neural network.\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "The problem of Binary classification, is one of finding a mapping from a feature vector $\\mathbf{x}$ to a target $y$ - y must be 1 or 0 - from examples in a historical dataset.\n",
    "\n",
    "The 'hello world' problem in machine learning (at least for computer vision) is one of learning a mapping of an image ($x$) as either having a cat in the image ($y=1$) or not $y=0$.\n",
    "\n",
    "An image is an array of numbers (from 0 to 255) made up of 3 channels (one for each of Red, Green and Blue) across the height and width of the image. Each number is the intensity of the pixel in each color channel at a given location in the image. We can think of this as a 3 dimensional array, $A$. The way to get a feature vector $x$ from $A$ is to unroll, unravel, or reshape $A$ into a vector. \n",
    "\n",
    "For an image that is $64 x 64$ pixels, there are $64 x 64 x 3 =1228$ numbers that make up the feature vector. The length of the feature vector is referenced as $n = n_{X}$.\n",
    "\n",
    "Let's say we have $m = m_{train}$ examples of images with cats and without cats in them. Then we can define the data as a feature matrix $X$, target vector $Y$ as below:\n",
    "\n",
    "$$\n",
    "X = \\left [ x^{(1)}, \\ldots,  x^{(m)} \\right ]\n",
    "$$\n",
    "$$\n",
    "Y = \\left [ y^{(1)}, \\ldots,  y^{(m)} \\right ]\n",
    "$$\n",
    "\n",
    "In this setting `X` is a matrix with `X.shape = (m, n)`, and `Y` is a row vector with `Y.shape=(1, m)`. This mean that the individual $x^{(i)}$ are column vectors.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "In the logistic regression model, we specify that a linear combination of the features, $x$, as inputs to a non-linear function, in this case the sigmoid function. It produces an estimate of the chance that $y$ is 1, $P(y=1|x) = \\hat{y}$, as opposed to explicitly trying to model the target as as function of $x$.\n",
    "\n",
    "If we tried to model the target, we can't get a smooth function that maps to 1 or 0 only. By using the sigmoid we ensure that the probability of $y$ being 1 is explicitly modelled, and bounded between 0 and 1. The sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\left( 1 + e^{-z} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "We will see later, that for hidden layers it might make sense to allow the non-linear function, also called the activation function, to be a hyperparameter.\n",
    "\n",
    "If the linear combination is a large number, then $P(y=1|x)$ gets very close to 1. Using a certain framework (Maximum A Prior or MAP) we can predict that in such cases y will be 1.\n",
    "\n",
    "If the linear combination is a large negative number, then  $P(y=1|x)$ gets very close to 0. Using the MAP framework we can predict that in such cases y will be 1.\n",
    " \n",
    "If the linear combination is close to 0, then $P(y=1|x)$ is close to 0.5. Under MAP we can make a prediction either side of 0.5 (> 0.5 to 1, <0.5 to 0), but we can't be very sure about which it is.\n",
    "\n",
    "#### with explicit bias `b`\n",
    "\n",
    "Using parameters $\\mathbf{w} = [w_1, \\ldots, w_n]$ (a column vector) and b, a scalar bias parameter. We model $\\hat{y}$ as $\\hat{y} = \\sigma(w^{T}x + b)$.\n",
    "\n",
    "#### with implicit bias $\\theta_0$\n",
    "\n",
    "If we extend x by adding $x_0=1$, set an example feature as $x = (x_0, \\ldots, x_n)$ and define $(w, b) = \\mathbf{\\theta}$ - still a column vector, with $\\theta_0 = b$ then it we can write this model as:\n",
    "\n",
    "$\\hat{y}$ as $\\hat{y} = \\sigma(\\mathbf{\\theta}^{T}x)$\n",
    "\n",
    "This formulation is not preferred because by having the bias term explicitly available, some of the future arithmetic becomes easier to understand.\n",
    "\n",
    "### Logistic Regression Cost Function\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "### Derivatives\n",
    "### More Derivatives Examples\n",
    "### Computation Graph\n",
    "### Derivatives with a Computation Graph\n",
    "### Gradient Descent on `m` examples\n",
    "\n",
    "## Python and Vectorization\n",
    "### Vectorization\n",
    "### More Vectorization Examples\n",
    "### Vectorizing Logistic Regression\n",
    "### Vectorizing Logistic Regressions Gradient Output\n",
    "### Broadcasting in Numpy\n",
    "### A Note on Numpy Vectors\n",
    "### Explanation of Logistic Regression Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
