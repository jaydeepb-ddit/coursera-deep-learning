{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2 Notes\n",
    "\n",
    "It's possible to cast a logistic regression as a simple neural network.  This is what we do here, along with introducting the backward pass and forward pass as part of the computation graph. Finally, we introduce a vectorized implementation and demonstrate its improved efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as a Neural Network\n",
    "Casting logistic regression as a neural network.\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "The problem of Binary classification, is one of finding a mapping from a feature vector $\\mathbf{x}$ to a target $y$ - y must be 1 or 0 - from examples in a historical dataset.\n",
    "\n",
    "The 'hello world' problem in machine learning (at least for computer vision) is one of learning a mapping of an image ($x$) as either having a cat in the image ($y=1$) or not $y=0$.\n",
    "\n",
    "An image is an array of numbers (from 0 to 255) made up of 3 channels (one for each of Red, Green and Blue) across the height and width of the image. Each number is the intensity of the pixel in each color channel at a given location in the image. We can think of this as a 3 dimensional array, $A$. The way to get a feature vector $x$ from $A$ is to unroll, unravel, or reshape $A$ into a vector. \n",
    "\n",
    "For an image that is $64 x 64$ pixels, there are $64 x 64 x 3 =1228$ numbers that make up the feature vector. The length of the feature vector is referenced as $n = n_{X}$.\n",
    "\n",
    "Let's say we have $m = m_{train}$ examples of images with cats and without cats in them. Then we can define the data as a feature matrix $X$, target vector $Y$ as below:\n",
    "\n",
    "$$\n",
    "X = \\left [ x^{(1)}, \\ldots,  x^{(m)} \\right ]\n",
    "$$\n",
    "$$\n",
    "Y = \\left [ y^{(1)}, \\ldots,  y^{(m)} \\right ]\n",
    "$$\n",
    "\n",
    "In this setting `X` is a matrix with `X.shape = (m, n)`, and `Y` is a row vector with `Y.shape=(1, m)`. This mean that the individual $x^{(i)}$ are column vectors.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "In the logistic regression model, we specify that a linear combination of the features, $x$, as inputs to a non-linear function, in this case the sigmoid function. It produces an estimate of the chance that $y$ is 1, $P(y=1|x) = \\hat{y}$, as opposed to explicitly trying to model the target as as function of $x$.\n",
    "\n",
    "If we tried to model the target, we can't get a smooth function that maps to 1 or 0 only. By using the sigmoid we ensure that the probability of $y$ being 1 is explicitly modelled, and bounded between 0 and 1. The sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\left( 1 + e^{-z} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "We will see later, that for hidden layers it might make sense to allow the non-linear function, also called the activation function, to be a hyperparameter.\n",
    "\n",
    "If the linear combination is a large number, then $P(y=1|x)$ gets very close to 1. Using a certain framework (Maximum A Prior or MAP) we can predict that in such cases y will be 1.\n",
    "\n",
    "If the linear combination is a large negative number, then  $P(y=1|x)$ gets very close to 0. Using the MAP framework we can predict that in such cases y will be 1.\n",
    " \n",
    "If the linear combination is close to 0, then $P(y=1|x)$ is close to 0.5. Under MAP we can make a prediction either side of 0.5 (> 0.5 to 1, <0.5 to 0), but we can't be very sure about which it is.\n",
    "\n",
    "#### with explicit bias `b`\n",
    "\n",
    "Using parameters $\\mathbf{w} = [w_1, \\ldots, w_n]$ (a column vector) and b, a scalar bias parameter. We model $\\hat{y}$ as $\\hat{y} = \\sigma(w^{T}x + b)$.\n",
    "\n",
    "#### with implicit bias $\\theta_0$\n",
    "\n",
    "If we extend x by adding $x_0=1$, set an example feature as $x = (x_0, \\ldots, x_n)$ and define $(w, b) = \\mathbf{\\theta}$ - still a column vector, with $\\theta_0 = b$ then it we can write this model as:\n",
    "\n",
    "$\\hat{y} = \\sigma(\\mathbf{\\theta}^{T}x)$\n",
    "\n",
    "This formulation is not preferred because by having the bias term explicitly available, some of the future arithmetic becomes easier to understand.\n",
    "\n",
    "### Logistic Regression Cost Function\n",
    "\n",
    "It is desirable that as we learn the parameters of the logistic regression that for each of the examples in our dataset, we try for each example $i$, to make $\\hat{y^{(i)}}$ as close to $y^{(i)}$ as possible. We can measure the loss, or distance, of each estimate $\\hat{y^{(i)}}$ from the ideal target $y^{(i)}$ using a function $L$. Some candidate $L$ functions are:\n",
    "\n",
    "1. $L(y, \\hat{y}) = |y - \\hat{y}|$\n",
    "\n",
    "1. $L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$\n",
    "\n",
    "1. $L(y, \\hat{y}) = - \\left [ y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\right ]$\n",
    "\n",
    "Of these, the third loss function has many desirable properties such as: \n",
    "\n",
    "* differentiable, smooth and convex with unique optima\n",
    "* a natural mapping to the negative log-likelihood (discussed later)\n",
    "\n",
    "The first is the most compelling, later we will explain second part - the negative log-likelihood. So we can change the parameters $w$ and $b$ until we reach a low loss.\n",
    "\n",
    "Since we want to reduce the loss for all the dataset, then we can use the individual losses to define an averaged loss function over all examples. This is called the cost function J of the dataset. That is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{m}\\sum_{i=1}^{m} L(y^{(i)}, \\hat{y^{(i)}})\n",
    "=  \\frac{-1}{m}\\sum_{i=1}^{m}  \\left [ y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\right ]\n",
    "$$\n",
    "\n",
    "Because of our choice of loss function (convex and smooth), we can guarantee that J has a single global optima. \n",
    "\n",
    "By iterating using a Taylor series approximation (known as gradient descent) we converge to the optimal parameters that minimize the cost function. \n",
    "\n",
    "These parameters define a classification rule, or classifier to model the binary data.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "### More Derivatives Examples\n",
    "\n",
    "### Computation Graph\n",
    "\n",
    "### Derivatives with a Computation Graph\n",
    "\n",
    "### Gradient Descent on `m` examples\n",
    "\n",
    "## Python and Vectorization\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "### More Vectorization Examples\n",
    "\n",
    "### Vectorizing Logistic Regression\n",
    "\n",
    "### Vectorizing Logistic Regressions Gradient Output\n",
    "\n",
    "### Broadcasting in Numpy\n",
    "\n",
    "### A Note on Numpy Vectors\n",
    "\n",
    "### Explanation of Logistic Regression Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
