{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deep L Layer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Notation: L Layers Network and Hidden (L-1) Layer Networks\n",
    "\n",
    "We don't count the input layer when deciding how many layers a neural network architecture has. Some refer to the input layer as Layer $0$. \n",
    "\n",
    "We usually refer to a $L$ Layer network as one having $L$ layers not including the input layer. \n",
    "\n",
    "Take one off that and we refer to $L-1$ hidden layers - as in layers whose values are never observed, only inferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Shallow and Deep Layer Networks\n",
    "\n",
    "Talking of shallow and deep, it's a matter of relativity. I would tend to think of more than 3/4 layers as deep. One to two layers is considered a shallow network.\n",
    "\n",
    "![Example Network](https://kharshit.github.io/img/deep_neural_net.png)\n",
    "\n",
    "101 layer networks have been trained successfully - such as the Microsoft ResNet-101 for computer vision problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation In A Deep Network\n",
    "\n",
    "Forward propagation is easy. We'll just set out some notation and then the recurrence propagation equations here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "- $n^{[i]}$ is the number of neurons at layer $i$, $i=1, \\ldots, L$. Note that $n^{[0]} = n_x = n$ is the number of features for each example.\n",
    "\n",
    "- $w^{[i]}_{j, k}$ is the $k^{th}$ weight (for $k=0,\\ldots,n^{[i-1]}$), a scalar that is the weighting put on $k^{th}$ input of layer $i$ which goes into the linear combination of neuron $j$.\n",
    "\n",
    "- $b^{[i]}_{j}$ is a scalar, that is the bias of the linear combination in the neuron $j$ at layer $i$.\n",
    "\n",
    "- $z^{[i]}_{j}$ is a scalar. It is the linear combination in neuron $j$ at layer $i$, this is the product-sum of the weights and inputs to neuron $j$ at layer $i$ plus the bias, $b^{[i]}_{j}$. The inputs are $a^{[i-1]}_{1}, \\ldots, a^{[i-1]}_{n^{[i-1]}}$. That is:\n",
    "\n",
    "- $z^{[i]}_{j} = ({w^{[i]}_{j}})^T \\cdot (a^{[i-1]}_{0}, \\ldots, a^{[i-1]}_{i}) + b^{[i]}_{j} $\n",
    "\n",
    "- $g^{[i]}_{j}$ is the activation function of the linear combination ($z^{[i]}_{j}$) in neuron $j$ at layer $i$ - denoted by  $g^{[i]}_{j}$. \n",
    "\n",
    "- $a^{[i]}_{j}$ is $g^{[i]}_{j}(z^{[i]}_{j})$ the scalar activated value which neuron $j$ of layer $i$ outputs - this is the input to the next layer, $(i+1)$. There are $n^{[i]}$ such activations.\n",
    "\n",
    "#### Compressed Vector Notation\n",
    "\n",
    "These can be combined into matrices and vectors over the neurons in each layer, to make the notation more compact. The quantities that interest us (and are important for forward and backward propagation) are matrices W, Z, A, B and G. There is one for each layer $i$, for $i$ ranging over the $L$ layers.\n",
    "\n",
    "##### Weights and Bias\n",
    "\n",
    "- $\\mathbf{n} = [n^{[0]}, n^{[1]}, ...,n^{[L]}]$ is the vector giving the number of neurons in each layer of the network architecture. \n",
    "\n",
    "- $w^{[i]}_{j}$ is the vector of weights on layer i which maps the inputs of neuron j to the linear combination. The shape of  $w^{[i]}_{j}$ is $(n^{[i-1]}, 1)$, that is to say it is a column vector of length $n^{[i-1]}$. There are $n^{[i]}$ such vectors.\n",
    "\n",
    "- $W^[i]$ is the matrix of vector weights for layer $i$. The $j^{th}$ row vector corresponds to $(w^{[i]}_{j})^T$. In other words, $w^{[i]}_{j, k}$ is the entry for the $j^{th}$ row and $k^{th}$ column of $W^{[i]}$. The shape of $W^{[i]}$ is $(n^{[i]}, n^[i-1])$\n",
    "\n",
    "- $B^[i]$ is the column vector of biases for layer $i$. It has a shape that is $(n^{[i]},1)$. The $j^{th}$ component of $B^[i]$ is the bias parameter of the neuron $j$ on layer $i$.\n",
    "\n",
    "###  Z, G, A: With Single Example Input\n",
    "\n",
    "For each layer $i$ there are Z, G and A vectors.\n",
    "\n",
    "- $Z_j^{[i]}$: The $j^th$ component of column vector $Z^{[i]}$. This is the linear combination that goes through the activation function in neuron $j$ of layer $[i]$. For a single example as input, $Z^{[i]}$ has shape $(n^{[i]}, 1)$.\n",
    "\n",
    "- $G_j^{[i]}$: The $j^th$ component of column vector $G^{[i]}$. This is the activation function definition in neuron $j$ of layer $[i]$. For a single example as input, $G^{[i]}$ has shape $(n^{[i]}, 1)$.\n",
    "\n",
    "- $A_j^{[i]}$: The $j^th$ component of column vector $A^{[i]}$. This is the activated value in neuron $j$ of layer $[i]$. For a single example as input, $A^{[i]}$ has shape $(n^{[i]}, 1)$. This is also defined by the equation: $A_j^{[i]} = $G_j^[(i)]$(Z_j^[(i)])$. For a single example as input, $Z^{[i]}$ has shape $(n^{[i]}, 1)$.\n",
    "\n",
    "As we will see later, setting $a^{[0]} = x$, an individual example, produces the vectorized form of recurrence relation with matrices for A, G, and Z.\n",
    "\n",
    "\n",
    "###  Z, G, A: With Full Feature Matrix As Input\n",
    "\n",
    "These work as above only the input is a feature matrix, whose columns are individual examples. The rows are for each kind of feature. As below the vectors become matrices by adding more columns, one for each example, (Recall that $m = n^{[0]}$). \n",
    "\n",
    "- $Z_j^{[i] (k)}$: The $j^{th}$ component of column vector $Z^{[i]}$. This is the linear combination that goes through the activation function in neuron $j$ of layer $[i]$. For the feature matrix as input, $Z^{[i]}$ has shape $(n^{[i]}, n^{[0]})$.\n",
    "\n",
    "- $G_j^{[i] (k)}$: The $j^th$ component of column vector $G^{[i]}$. This is the activation function definition in neuron $j$ of layer $[i]$. For the feature matrix as input, $G^{[i]}$ has shape $(n^{[i]}, n^{[0]})$.\n",
    "\n",
    "- $A_j^{[i] (k)}$: The $j^{th}$ row of the $k^{th}$ column of matrix $A^{[i]}$. This is the activated value in neuron $j$ of layer $[i]$ of the $k^{th}$ example. For a single example as input, $A^{[i]}$ has shape $(n^{[i]}, 1)$. This is also defined by the equation: $A_j^{[i]} = G_j^{[(i)]}(Z_j^{[(i)]})$. For the feature matrix as input, $Z^{[i]}$ has shape $(n^{[i]}, n^{[0]})$.\n",
    "\n",
    "\n",
    "As we will see later, setting $A^{[0]} = X$, the feature matrix, produces the vectorized form of recurrence relation with matrices for A, G, and Z.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Your Matrix Dimensions Correct\n",
    "\n",
    "A good sanity check is to confirm the dimensions of parameter matrices. Below we note the dimensions in unvectorized as well as vectorized implementations. Note that  $m = n^{[0]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unvectorized Dimensions\n",
    "Here we list `variable:shape` for unvectorized implementations. Shape is given as a numpy style tuple (dim1, dim2).\n",
    "\n",
    "- $w^{[l]}:(n^{[l-1]}, n^{[l]})$\n",
    "\n",
    "- $b^{[l]}:(n^{[l]}, 1)$\n",
    "\n",
    "- $dw:(n^{[l-1]}, n^{[l]})$\n",
    "\n",
    "- $db:(n^{[l]}, 1)$\n",
    "\n",
    "- $z:(n^{[l]}, 1)$\n",
    "\n",
    "- $a:(n^{[l]}, 1)$\n",
    "\n",
    "- $dz:(n^{[l]}, 1)$\n",
    "\n",
    "- $da:(n^{[l]}, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Dimensions\n",
    "Here we list `variable:shape` for vectorized implementations. Shape is given as a numpy style tuple (dim1, dim2,..).\n",
    "- $w^{[l]}:(n^{[l-1]}, n^{[l]})$\n",
    "\n",
    "- $b^{[l]}:(n^{[l]}, n^{[0]})$\n",
    "\n",
    "- $dw:(n^{[l-1]}, n^{[l]})$\n",
    "\n",
    "- $db:(n^{[l]},  n^{[0]})$\n",
    "\n",
    "- $z:(n^{[l]}, n^{[0]})$\n",
    "\n",
    "- $a:(n^{[l]}, n^{[0]})$\n",
    "\n",
    "- $dz:(n^{[l]}, n^{[0]})$\n",
    "\n",
    "- $da:(n^{[l]}, n^{[0]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Representations\n",
    "\n",
    "Deep representations tend to have a tree like structure. They tend to non-increase (and eventually reduce) the number of neurons until the final output vector at the final layer. There has been discussion about why such representations are so effective as opposed to a shallow representation that might use only one hidden layer. (In theory, a one hidden layer network can model any non-linear function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Building Up Local Features\n",
    "\n",
    "Start by learning simple features like edges, then learn small similar surfaces (like eyes, nose, lips or phonemes such as \"ca\", \"ta\"), then more complex compositions of those (such as faces, words like \"cat\"). Some biological justification if we believe that artificial neural networks do similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reducing Parameters By Networking Neurons\n",
    "\n",
    "Functions with $2^n$ parameters can be learned with $\\log(n)$ using deep networks. Using just 1 layer hidden network, we can also learn such a function, but we will need  $2^n$ neurons to learn such a function. This is the power of having many connections, they expand the possibilities much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks of Deep Neural Networks\n",
    "\n",
    "The forward propagation to calculate the final layer and the backward propagation to calculate the gradient repeat some calculations. \n",
    "\n",
    "![BackProp and ForwardProp working with Gradient Descent](https://cdn-images-1.medium.com/max/1600/1*I_fAEeJbT_G4hGkXBJ-PcQ.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Terms From The Forward Propagation\n",
    "\n",
    "Starting with intial parameters one can propagate and cache $A$'s and $Z$'s ($W$'s and $B's$ are available via a higher scope - one could cache them too) in each layer and then use them to calculate the gradient at these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cached Terms For The Backward Propagation\n",
    " As we are backward propagating the gradient (initialized with the value of the loss function), we can use the values of $A$'s and $Z$ from the cache ($𝑊$'s and $𝐵$′𝑠 are available via a higher scope - one could cache them too) and update the cached values of the gradients of the parameters. These can then be used to update the parameters which are then used again in the next forward propagation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backward Propagation\n",
    "\n",
    "There are a set of recurrence relations which allow us to: \n",
    "\n",
    "- propagate parameters forwards and get outputs at the final layer. \n",
    "\n",
    "- propagate parameters and outputs(from above), backwards via gradient equations and get the gradient of the loss function wrt the parameters.\n",
    "\n",
    "We can use the caching described earlier to speed the calculation up, but it is not strictly necessary. \n",
    "\n",
    "Below we list the forward and backward propagation equations, in both unvectorized and vectorized forms.\n",
    "\n",
    "### Forward Propagation Equations\n",
    "\n",
    "#### Unvectorized\n",
    "\n",
    "#### Vectorized\n",
    "\n",
    "### Backward Propagation Equations\n",
    "\n",
    "#### Unvectorized\n",
    "\n",
    "#### Vectorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Hyper-parameters\n",
    "\n",
    "For any representation, there are a set of numbers to be learned. These are split into two parts:\n",
    "\n",
    "- Those which are preset before learning from data, and are repeatedly evaluated using some strategy to tune  the best set of such of presets.\n",
    "\n",
    "- Those which can be learned directly from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "The parameters are learned from the model specified. Usually, following some usually iterative process to minimize model loss, a best set of parameters is learned - given the presets of the hyperparameters. By tuning the hyperparameters best generalization performance can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The hyperparameters need to be tuned by one of several methods:\n",
    "\n",
    "* grid search: expensive but exhausts a certain space. \n",
    "\n",
    "* random search: might be more effective.\n",
    "\n",
    "* Bayesian hyper-optimization: Using a framework, finding a balance between exploring for new candidates and exploiting what is known about the hyperparameters.\n",
    "\n",
    "They can be considered as a kind of empirical Bayes, where their distribution is learned from repeatedly setting and then fitting the data. \n",
    "\n",
    "The preferred way to select between hyperparameters is to use the same loss function evaluated on data (validation or dev set of data) which has not been used to learn the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does This Have To Do With The Brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity With Dendrites, Soma and Axons\n",
    "\n",
    "There is some superficial similarity between how a real neuron works and an artificial neuron. \n",
    "\n",
    "![Dendrites, Soma and Axons](http://oerpub.github.io/epubjs-demo-book/resources/1206_The_Neuron.jpg)\n",
    "\n",
    "Dendrites are like the inputs to a neuron. \n",
    "\n",
    "The soma is like the linear combination and activation of the inputs of artificial neuron. \n",
    "\n",
    "The Axon is like the output of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marketing\n",
    "\n",
    "There's a *cool* aspect to re-branding neural networks with more layers as \"Deep Learning\". Neural nets have been around since the 1950s, recasting them with a new name helps to freshen the image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
