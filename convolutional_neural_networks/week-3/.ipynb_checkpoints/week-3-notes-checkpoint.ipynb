{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Notes\n",
    "\n",
    "## Finding Objects \n",
    "\n",
    "IN computer vision, there are three major tasks related to finding objects:\n",
    "\n",
    "* Object classification: Given a well centered and cropped image with only one dominating object, the task is to classify what the object is, from a pre-defined set of possibilities.\n",
    "\n",
    "* Object localization: Classify and localize (with bounding boxes or similar tools) the single instance of an object in an image.\n",
    "\n",
    "* Object detection: Classify and localize (with bounding boxes or similar tools) multiple instances of object in an image.\n",
    "\n",
    "There is also landmark detection and the related concept of pose estimation. The objective is to estimate points which are related - such as eyes, hairline or a person standing, crouching. This is the technology that powers SNAP filters and related products.\n",
    "\n",
    "\n",
    "The ideas in these tasks are related and generalize to build up to object detection.\n",
    "\n",
    "### Object Classification\n",
    "\n",
    "Object classification is the task of looking at an image and classifying the presence or not of an object.\n",
    "\n",
    "### Object Localization\n",
    "\n",
    "Object localization is to look at an image and classify an object as one of some finite predetermined classes and finding its location or co-ordinates in the image. One popular way to locate an object is to give the co-ordinates of a bounding box or rectangle around it.\n",
    "\n",
    "We tend to use the convention that (0,0) is the top left corner of an image and (1,1) is the bottom right corner. Any points in between are proportionally set.\n",
    "\n",
    "Specifically, an image is passed through a neural network (usually convolutional) and the head or final output is a longer vector than see before. For a task that is to find the prescence of, class of and co-ordinates of an object, we need:\n",
    "\n",
    "* the presence of an object, p (binary or probability response)\n",
    "\n",
    "* the co-ordinates of the object, given as:\n",
    "    - midpoint of rectangle, x\n",
    "    - midpoint of rectangle, y\n",
    "    - height of rectangle\n",
    "    - width of rectangle\n",
    "\n",
    "* the classification vector - with one hot encoding as 1 for the class that is predicted, and zero for all others. These are denoted by $c_i$. Alternatively, the classes can be modeled as probabilities.\n",
    "\n",
    "If $p=0$, then the remaining vector numbers are ignored, particularly when using a loss function to train the model.\n",
    "\n",
    "#### Loss Function For Object Localization\n",
    "\n",
    "A crude way to model the loss of misclassification would be to use binary response with squared error for the presence indicator.\n",
    "\n",
    "The same can be used for class probabilities as well as for the co-ordinates of the midpoint, length and width of the bounding box.\n",
    "\n",
    "Then summed together, we get a loss function that can be used for training object localization networks.\n",
    "\n",
    "In practice however, one would use a different loss function. For the classification tasks of presence indication and class label, cross entropy loss is generally seen as a better way to process outputs. That is, logistic loss for the presence indicator and multinomial log-likelihood for the classification label.\n",
    "\n",
    "### Landmark Detection\n",
    "\n",
    "The idea of outputting regression vectors from a neural network applied to an image is powerful. It can be used to detect landmarks in images. Of course, significant labeling of training images is needed to train a useful model.\n",
    "\n",
    "#### Face Mapping\n",
    "\n",
    "To map the core features of a face: the mouth, eyes, nose, lips - we need to locate around 64 points in the x and y co-ordinates.\n",
    "\n",
    "#### Pose Estimation\n",
    "\n",
    "To map the core features of a pose, we usually want to know the location of 32 points, corresponding to joints  - in the x and y co-ordinates.\n",
    "\n",
    "## Object Detection\n",
    "\n",
    "Object detection is like object localization, only here the task is to classify and locate (using bounding boxes) possibly many objects in a single image.\n",
    "\n",
    "By sliding windows of different sizes and strides across an image, we can apply the same localization method to each window throughout an image.\n",
    "\n",
    "One problem with this approach is the computational cost of working through all these windows at prediction time is very high. This is much harder now since neural networks have a high computational overhead themselves. \n",
    "\n",
    "In the past it was acceptable to repeatedly train classifiers because linear classifiers were dominant and are computationally cheap to use.\n",
    "\n",
    "Although it is possible to use wider windows to speed up performance there is a danger of missing the object or having a bounding box that is too large.\n",
    "\n",
    "One way to solve this is to use a convolutional implementation of sliding windows - which simultaneously predicts the classifier across all windows.\n",
    "\n",
    "### Convolutional Implementation Of Sliding Windows\n",
    "\n",
    "By making use of 1 by 1 convolutions instead of fully connected layers, then we can pass all the sliding windows of the same size through a ConvNet in one parallel computation - this helps to speed up significantly sliding windows detection.\n",
    "\n",
    "By applying 1 by 1 convolutions and inputting larger regions the parallel speed up helps materially.\n",
    "\n",
    "### YOLO Algorithm\n",
    "\n",
    "The YOLO Algorithm is a fast detection algorithm. It is a one stage detector in that it detects and classifies objects in one go. It does suffer from a few issues, including:\n",
    "\n",
    "* lower accuracy rates as compared to two stage detectors such as Faster R-CNN\n",
    "* weak classification of smaller objects in a image.\n",
    "\n",
    "However, it is still very popular. We list some of the components needed to implement YOLO:\n",
    "\n",
    "* Bounding Box Predictions\n",
    "\n",
    "* Intersection Over Union\n",
    "\n",
    "* NonMax Suppression\n",
    "\n",
    "* Anchor Boxes\n",
    "\n",
    "These are discussed in more detail, before YOLO itself is outlined. Note that many of these components can be used by other object detectors too.\n",
    "\n",
    "#### Bounding Box Predictions\n",
    "\n",
    "The YOLO paper doesn't use sliding windows to detect objects. Instead is applies a grid over the image. \n",
    "\n",
    "At training time, the grid is chosen so that inside each grid cell there is at most one object. Then the co-ordinates and classification are recorded in the label data. \n",
    "\n",
    "A ConvNet is then applied with the detection head to the grid cells. This can be done to cells over different grid sizes. Note how the learning happens in one ConvNet, avoiding training and inferring from multiple networks.\n",
    "\n",
    "There are some components that can be used to make the algorithm usable. These are discussed below.\n",
    "\n",
    "#### Intersection Over Union\n",
    "\n",
    "When there are many overlapping grid cells which indicate the presence of an object, we need a way to assign an object to just one of the cells. Intersection over Union (IoU) is one way.\n",
    "\n",
    "When comparing two cells we can measure how similar they are with the following formula:\n",
    "\n",
    "IoU(A,B) = $\\frac{Intersection(A, B)}{Union(A, B)}$ \n",
    "\n",
    "where $Intersection(\\cdot, \\cdot)$ and Union$(\\cdot, \\cdot)$ are the areas of the the intersection and union of the boxes.\n",
    "\n",
    "If $IoU(A, B) > \\tau$ for some $\\tau \\geq 0.5$, usually $\\tau = 0.5$ is common, then we can say that A and B are similar. \n",
    "\n",
    "It is used when deciding if a prediction A is close enough to the ground truth B to be a good fit.\n",
    "\n",
    "#### NonMax Suppression\n",
    "\n",
    "One problem with this grid approach to detecting images is that the object may be detected multiple times by different grids. NonMax suppression is one way to associate one object with one tightly bounding grid cell/bounding box.\n",
    "\n",
    "The strategy is as follows. \n",
    "\n",
    "1. Select the bounding box with the highest probability of an object.\n",
    "\n",
    "2. Suppress all other bounding boxes that have a high IoU, over a threshold $\\tau$, say $0.5$.\n",
    "\n",
    "3. Go back to 1 and pick the next highest probability box.\n",
    "\n",
    "In this way, all the NonMax boxes are suppressed.\n",
    "\n",
    "#### Anchor Boxes\n",
    "\n",
    "Anchor Boxes are a way to work with overlapping objects. It changes the head of the neural network to be larger so that multiple objects can be localized.\n",
    "\n",
    "It works well for detecting two objects in the same region, one of which is wide and the other long. However it doesn't work so well for three or more objects, or when both objects the same shape.\n",
    "\n",
    "#### YOLO: Bringing It All Together\n",
    "\n",
    "Let's now go through the YOLO algorithm. Firstly to compose the final head of the neural network do the following, note it's dimenision.\n",
    "\n",
    "grid_size by grid_size by anchor_boxes by  (5 + num_classes)\n",
    "\n",
    "The 5 comes from 4 coordinates for the bounding box and 1 classifier to indicate the presence or not of an object. Then run the ConvNet learning algorithm.\n",
    "\n",
    "For each cell you get anchor_boxes predictions. First suppress all low probability bounding boxes. Then cycle through each class and apply non-max suppression. What is left, should be the final set of predictions.\n",
    "\n",
    "## Region Proposals\n",
    "\n",
    "Another competing family of detectors is that two stage detectors - regional proposal followed by CNN. The are called two stage, because first they propose regions, and then they classify among those regions.\n",
    "\n",
    "Region Proposal algorithms are slower than YOLO and other one stage detectors, however they also have significantly higher accuracy.\n",
    "\n",
    "### R-CNN\n",
    "\n",
    "The original algorithm uses Selective Search algorithm (powered by Greedy Search) to propose 2000 regions to classify. Then, after warping the images are processed by a ConvNet. The output of this is given to multiclass SVM and bounding box regression to detect objects.\n",
    "\n",
    "This algorithm is slow, involving 2000 regions and the selective search algorithm does not do any learning so can propose bad regions. \n",
    "\n",
    "### Fast R-CNN\n",
    "\n",
    "Because of the speed bottleneck, a different algorithm was proposed by the same authors. First FastR-CNN trains many convolutional filters (or feature maps) over sliding windows. Then selective search algorithm is applied. Then these proposed regions are RoI pooled (a type of irregular size max pooling). \n",
    "\n",
    "These RoI features are then passed to fully connected layers, to output classification and regressions.\n",
    "\n",
    "It runs much faster than R-CNN at test time.\n",
    "\n",
    "### Faster R-CNN\n",
    "\n",
    "Similar to Fast R-CNN, the image is provided as an input to a convolutional network which provides a convolutional feature map. Instead of using selective search algorithm on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.\n",
    "\n",
    "\n",
    "### Focal Loss Detector\n",
    "\n",
    "The Focal loss detector is a one stage detector which uses a novel loss function to get higher accuracy performance while maintaining speed. It does this by reducing the loss incurred in fitting easy examples. It has a multiple scale and anchor box convolutional network which outputs bounding boxes and classification probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
