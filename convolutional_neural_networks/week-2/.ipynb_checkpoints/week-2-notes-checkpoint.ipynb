{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Look At Case Studies\n",
    "\n",
    "Case studies of famous networks are useful because there has usually been extensive research and implementations of them in the open source community. \n",
    "\n",
    "Engineers can use these well known network architectures with their implementations or finesse/modify trained versions to fit their problems and datasets.\n",
    "\n",
    "Many days/months of training is often needed to get the weight parameters of these networks. By modifying (by transfer learning for example) these pre-trained networks fast progress can be made.\n",
    "\n",
    "Also, there is a level of trust in their implementations as many researchers will be investigated the implementations and performance.\n",
    "\n",
    "A good way to learn make convnets is to see others built before. These case studies link to seminal papers and a satisfying intellectually. \n",
    "\n",
    "Many of the ideas here have and can be used for applications in areas other than computer vision also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Network\n",
    "\n",
    "There are three classic networks that have had significant impact on the computer vision community. These are:\n",
    "\n",
    "1. LeNet-5\n",
    "\n",
    "2. AlexNet\n",
    "\n",
    "3. VGG-16\n",
    "\n",
    "We will look at their architecture and link to open source implementations.\n",
    "\n",
    "### LeNet-5\n",
    "\n",
    "The LeNet-5 architecture solves the MNIST datasets digit classification problem. It was one of the first demonstrations of competition winning performance with convolutional neural networks. \n",
    "\n",
    "The MNIST database is comprised of 32 by 32 pixel grayscale images (1 channel) - 32x32x1. The MNIST database contains 60,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "The network has 4 hidden layers, or is a 5 layer network - hence the name LeNet-5 (also that the first author is Yann LeCun).\n",
    "\n",
    "![](LeNet_architecture.png)\n",
    "\n",
    "There are two convolutional layers - each made up of  a convolution combined with an average pooling. These are followed by two fully connected layers that link to a 10 output final layer.\n",
    "\n",
    "Note that the number of filters is also called the number of feature maps. There is a detail on the number of channels.\n",
    "\n",
    "Note this from [samuel](https://github.com/vlfeat/matconvnet/issues/457):\n",
    "\n",
    "\n",
    ">The number of feature channels and the number of filters refer to different things.\n",
    "\n",
    ">Suppose X is an input with size W x H x D x N (where N is the size of the batch) to a convolutional layer containing filters F (with size FW x FH x FD x K) in a network .\n",
    "\n",
    ">The number of feature channels D is the third dimension of the input X here (for example, this is typically 3 at the first input to the network if the input consists of colour images).\n",
    "The number of filters K is the fourth dimension of F.\n",
    "The two concepts are closely linked because if the number of filters in a layer is K, it produces an output with K feature channels. So the input to the next layer will have K feature channels.\n",
    "\n",
    "> There is a detailed explanation in Section 4.1 of the [manual](http://www.vlfeat.org/matconvnet/matconvnet-manual.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "##### Arithmetic By Layer Type\n",
    "\n",
    "For different types of layers, the number of parameters are calculated as:\n",
    "\n",
    "* Input Layer: 0 parameters.\n",
    "* Convolutional Layer: (FH*FW + 1) * K parameters\n",
    "* Average Pooling Layer: 0 parameters.\n",
    "* Fully Connected Layer:  $n^{L-1} * n^{L}$ parameters\n",
    "\n",
    "The tables below provides details:\n",
    "\n",
    "| layer_n | H | W | D | Activation Shape (Number Of Neurons) | Activation Size |\n",
    "|:--:| :--:| :--:| :--:| :--:| :--:|\n",
    "| 0 | 32| 32| 1| (32, 32, 1)|32 x 32 x 1 = 1024|\n",
    "| 1 | 28| 28| 6| (28, 28, 6)| 28 x 28 x 6 = 4704|\n",
    "| 2 | 14| 14| 6| (14, 14, 6)| 14 x 14 x 6 = 4704|\n",
    "| 3 | 10| 10| 16| (10, 10, 16)| 10 x 10 x 16 = 1600|\n",
    "| 4 | 5| 5| 16| (5, 5, 16)| 5 x 5 x 16 = 320|\n",
    "| 5 | 1| 1| 16| (1, 1, 120)| 1 x 1 x 120 = 120|\n",
    "| 6 | 1| 1| 84| (1, 1, 84)| 1 x 1 x 84 = 84|\n",
    "| 7 | 1| 1| 10| (1, 1, 10)| 1 x 1 x 10 = 10|\n",
    "\n",
    "\n",
    "| layer_n | fW x fH x fD | s | p| n_filters (K) | n_parameters |  notes | n_examples |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|0 | 0 x 0 x 0| 0| 0| 0| 1024| Input Image| 60,000|\n",
    "|1 | 5 x 5 x 1| 1| 0| 6| (5 x 5 + 1) x 6 = 156| Convolution 1| 60,000|\n",
    "|2 | 2 x 2 x 6| 2| 0| 6| 0| Average Pooling 1| 60,000|\n",
    "|3 | 5 x 5 x 6| 1| 0| 16| (5 x 5 + 1) x 16 = 416| Convolution 2| 60,000|\n",
    "|4 | 2 x 2 x 16| 2| 0| 120| 0| Average Pooling 2| 60,000|\n",
    "|5 | 1 x 1 x 120| 1| 0| 84| 120 x 416 + 1 = 48,001| Fully Connected 3| 60,000|\n",
    "|6 | 1 x 1 x 84| 1| 0| 10| 84 x 120 + 1 = 10,081| Fully Connected 4| 60,000|\n",
    "|7 | 2 x 2 x 10| 1| 0| 1| 10 x 84 + 1 =  841| SoftMax| 60,000|\n",
    "\n",
    "\n",
    "In total, there are: 156 + 416 + 48,001 + 10,081 + 841 = 59,495\n",
    "\n",
    "Or almost 60K parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details\n",
    "\n",
    "Some details that were popular back then, but less used or relevant now are:\n",
    "\n",
    "* average pooling\n",
    "\n",
    "* sigmoid/tanh non-linearities\n",
    "\n",
    "* non-linearity after pooling\n",
    "\n",
    "sections 2 and 3 are the most important, other sections refer to the above and other ideas that are not widely used anymore - for example graph transformers.\n",
    "\n",
    "Notice how, generally, the height and width of the convolutional layers decreases while the number of channels increases as we move through the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "This network demonstrated the strength of deep learning to the computer vision community in 2012. It produced state of the art results on the well mined ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "AlexNet has 5 convolutional layers and 3 fully connected layers. It has around ~60M parameters.\n",
    "\n",
    "![](AlexNet_architecture.png)\n",
    "\n",
    "AlexNet introduced a number of features that were not present in other convolutional networks. These include:\n",
    "\n",
    "* ReLU activation function\n",
    "\n",
    "* Using DropOut to deal with overfitting instead of regularization\n",
    "\n",
    "* Overlap pooling to reduce the size of the network - larger pooling windows have less capacity.\n",
    "\n",
    "The other important feature was that these networks were trained using GPUs. This made them train much much faster than just on CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "Here is the parameter table of the AlexNet architecture.\n",
    "\n",
    "\n",
    "| layer_n | H | W | D | Activation Shape (Number Of Neurons) | Activation Size |\n",
    "|:--:| :--:| :--:| :--:| :--:| :--:|\n",
    "| 0 | 32| 32| 1| (32, 32, 1)|32 x 32 x 1 = 1024|\n",
    "\n",
    "\n",
    "| layer_n | fW x fH x fD | s | p| n_filters (K) | n_parameters |  notes | n_examples |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|0 | 0 x 0 x 0| 0| 0| 0| 1024| Input Image| 60,000|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "This architecture is very popular as a base to transfer learn or build more advanced heads on networks with. It has a simple repeated architecture and number of channels (feature maps) increasing in powers of two, which height and width also reducing in powers of 2.\n",
    "\n",
    "\n",
    "It does have many more parameters than AlexNet, at around ~140M parameters.\n",
    "\n",
    "\n",
    "![](VGG16_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "\n",
    "Here is the parameter table of the VGG-16 architecture.\n",
    "\n",
    "\n",
    "| layer_n | H | W | D | Activation Shape (Number Of Neurons) | Activation Size |\n",
    "|:--:| :--:| :--:| :--:| :--:| :--:|\n",
    "| 0 | 32| 32| 1| (32, 32, 1)|32 x 32 x 1 = 1024|\n",
    "\n",
    "\n",
    "\n",
    "| layer_n | fW x fH x fD | s | p| n_filters (K) | n_parameters |  notes | n_examples |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|0 | 0 x 0 x 0| 0| 0| 0| 1024| Input Image| 60,000|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNets\n",
    "\n",
    "ResNets make use of \"skipped connections\", that is they allow for a main path as well a as short cut to link layers that are more than one connection away. The authors call these skip connected layers \"blocks\", whereas the earlier seens connected layers are seen as \"plain\" networks.\n",
    "\n",
    "**Plain Blocks**\n",
    "\n",
    "**ResNet Blocks: Skipped Connections**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Parameters\n",
    "\n",
    "\n",
    "\n",
    "| layer_n | H | W | D | Activation Shape (Number Of Neurons) | Activation Size |\n",
    "|:--:| :--:| :--:| :--:| :--:| :--:|\n",
    "| 0 | 32| 32| 1| (32, 32, 1)|32 x 32 x 1 = 1024|\n",
    "\n",
    "\n",
    "\n",
    "| layer_n | fW x fH x fD | s | p| n_filters (K) | n_parameters |  notes | n_examples |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|0 | 0 x 0 x 0| 0| 0| 0| 1024| Input Image| 60,000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ResNets Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network In Network\n",
    "\n",
    "\"Network In A Network\" is just applying 1 by 1 filters to convolve volumes. In particular, if a layer has $(H, W, D)$ dimensions then K filters of dimensions $(1, 1, D)$ convolved with it result in a layer $(H, W, K)$.\n",
    "\n",
    "This has two well known uses:\n",
    "\n",
    "* to go from a volume to a full connected layer.\n",
    "\n",
    "* to \"bottleneck\" convolve in Inception networks - which reduces the number of parameters needed in Inception networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Module Motivation\n",
    "\n",
    "The motivation for inception blocks is that it is not clear what form a hidden layer should take:\n",
    "\n",
    "* Should it be a convolution? If so, what size of filter should it take? 1 by 1, 3 by 3 or 5 by 5?\n",
    "\n",
    "* Should it be a max pooling layer? \n",
    "\n",
    "INCEPTION_IMAGE\n",
    "\n",
    "Well, what if we allow all of these filters to be applied and generate a much higher number of feature maps (hence we need to go deeper meme from the movie inception - which also explains the name) - more correctly inception blocks go wider with many more feature maps.\n",
    "\n",
    "![](deeper_meme.jpeg)\n",
    "\n",
    "One problem is the computational cost (in the number of parameters to be estimated) can explode to be infeasible. A solution to this issue, presented by the authors is to have a bottleneck layer - which shrinks the volume using 1 by 1 convolutions with a smaller number of filters and then increases it back up by applying the desired filter sizes.\n",
    "\n",
    "BOTTLENECK_IMAGE\n",
    "\n",
    "In this way, a layer can train many feature maps, while controlling the number of parameters. This sets us up nicely to look at inception layers and inception networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception Module Based Network\n",
    "\n",
    "The Inception blocks produced state of the art performance on the ImageNet competition using 22 layer architecture called the GoogLeNet - in homage to Yann LeCun. \n",
    "\n",
    "In addition, due to its 22 layers, the network is exposed to vanishing gradients. The authors found a side auxiliary loss function - which uses earlier layers to predict the target, it carries less weight than the final loss function and has no inferential value, but stops vanishing gradients from being a great problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "\n",
    "| layer_n | H | W | D | Activation Shape (Number Of Neurons) | Activation Size |\n",
    "|:--:| :--:| :--:| :--:| :--:| :--:|\n",
    "| 0 | 32| 32| 1| (32, 32, 1)|32 x 32 x 1 = 1024|\n",
    "\n",
    "\n",
    "\n",
    "| layer_n | fW x fH x fD | s | p| n_filters (K) | n_parameters |  notes | n_examples |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|0 | 0 x 0 x 0| 0| 0| 0| 1024| Input Image| 60,000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Open Source Implementation\n",
    "\n",
    "One can usually google search for an implementation of a famous neural network architecture and find open source code - usually in your preferred framework. Nowadays the two competing frameworks are:\n",
    "\n",
    "* Tensorflow - mature with dozens of APIs and traditionally in a computational graph style. An eager execution mode is more Pythonic in that it runs line by line without compilation, even to run on GPUs or specialized chipsets. Tensorflow is supported by [Google](www.google.com).\n",
    "\n",
    "* PyTorch is popular in research groups and universities because it has fewer APIs and is pythonic - without compilation of computational graphs, and still run on GPUs. PyTorch is supported by [Facebook](www.facebook.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "There are many types of transfer learning, all of which take a pre-trained network (from some possibly related dataset) - that is some configuration of weight values and some other data along with a supervised learning task. Starting with this, one can do one or more of the following:\n",
    "\n",
    "* remove final layer(s), add new layers.\n",
    "\n",
    "* freeze some or all weights in later/all layers.\n",
    "\n",
    "* unfreeze all weights.\n",
    "\n",
    "Then, a learning algorithm can use the new architecture with given weights (new weights would be randomly generated, pre-trained weights used where available) and learn from the new dataset to target the supervised learning problem at hand.\n",
    "\n",
    "There are flags in most of the popular frameworks which allow transfer learning to take place.\n",
    "\n",
    "### When To Use Transfer Learning\n",
    "\n",
    "Transfer learning makes sense when the number of inputs to a problem are large and training would take a long time. Also if the available data and compute for the problem at hand is small, then transfer learning makes good sense. It helps if the problem to be solved is similar in some respects to the one the pre-trained model worked for.\n",
    "\n",
    "Where there is both a lot of data and compute, then transfer learning provides less value to the engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data Augmentation is to generate more data samples from the existing sample by applying thoughtful transformations to generate data derived from the original dataset.\n",
    "\n",
    "### Data Augmentation Techniques\n",
    "\n",
    "* Mirroring\n",
    "\n",
    "* Crops\n",
    "\n",
    "* Rotations\n",
    "\n",
    "* Shears\n",
    "\n",
    "* Color Shifting\n",
    "\n",
    "* PCA Color Shifting\n",
    "\n",
    "* Gaussian Noise\n",
    "\n",
    "* Test example cropping, mirroring, rotations, shears and color shifting and PCA color shifting to average predictions (probabilities for classification or target predictions for regression).\n",
    "\n",
    "### Parallel Data Augmentation\n",
    "\n",
    "As images are usually stored to disk and loaded into memory - an efficient way to augment the data is to:\n",
    "\n",
    "1. load an image into memory using the CPU - then have the CPU run a thread\n",
    "\n",
    "2. In the thread, generate the augmented data using the methods above. \n",
    "\n",
    "3. Follow this by sending all the augmented data to the GPU/CPU and have the learning algorithm update parameters of the network. \n",
    "\n",
    "4. Go back to 1 and load a new image.\n",
    "\n",
    "This can lead to dramatically faster training and performance of ML applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Of Computer Vision\n",
    "\n",
    "### Deep Learning in Computer Vision\n",
    "\n",
    "Deep Learning has been successfully applied to get state of the art results in a number of areas of computer vision. These include:\n",
    "\n",
    "* image recognition\n",
    "\n",
    "* speech recognition\n",
    "\n",
    "* speech translation\n",
    "\n",
    "* online advertising\n",
    "\n",
    "* logistics\n",
    "\n",
    "* image localization\n",
    "\n",
    "However, for some applications a lot of specialized architecture choices (hand engineering) need to be made. This is related to the amount of data available.\n",
    "\n",
    "\n",
    "### Spectrum Of Data Size Vs Hand Engineering\n",
    "\n",
    "The current state of computer vision is a mix of learning from enough examples on simple architectures to requiring specialized hand engineering (or 'hacks') to make progress. Below some points are marked on the spectrum.\n",
    "\n",
    "![](spectrum_data_vs_hand_engineering.png)\n",
    "\n",
    "\n",
    "One way to get around little data and avoid too much hand engineering is to make use of transfer learning. Transfer learning has good results with small data, by using the learning in other datasets with related architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competition Tips\n",
    "\n",
    "To do well in competitions, a few tricks are regularly used - which can boost performance by a few percentage points - which can be materially relevant to competitions.\n",
    "\n",
    "* Ensembling: There are a wide repetoire of ensembling methods all based around generating multiple models and averaging them in some way.\n",
    "\n",
    "* Test Case Augmentation: Generate many variations of the test case to get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Open Souce\n",
    "\n",
    "* Use open source implementations and frameworks - they have been tested and peer reviewed.\n",
    "\n",
    "* Sometimes exisiting implementations can be finessed or modified to achieve what you want. \n",
    "\n",
    "* Try to contribute when you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
