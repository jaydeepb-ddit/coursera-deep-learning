{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Face Recognition\n",
    "\n",
    "There are two aspects to facial systems in computer vision. These are:\n",
    "\n",
    "* Face Verification\n",
    "\n",
    "* Face Recognition\n",
    "\n",
    "In addition such systems tend to have a live human vs non-live image verification subsystem to ensure they are not being fooled by photos or videos.\n",
    "\n",
    "The focus here will be face verification. Face recognition can be generalized from the principles in face verification.\n",
    "\n",
    "One reason that vanilla ConvNets are not good at this task is the paucity of data. That is, this is a one shot learning task. One needs to learn to verify a person from just a small number of reference images (sometimes only 1) of that person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Learning\n",
    "\n",
    "One shot learning is about learning from few (or just one) data points. This is particularly relevant for the face verification problem. Here the task is to match to just one/few image(s). Historically deep and regular machine learning problems do not perform well with such few examples.\n",
    "\n",
    "One could try and learn a CNN with a final layer head that is the number of employees. However this doesn't work well since there isn't enough data to learn parameters robustly.\n",
    "\n",
    "Also each time a new employee joins or an old employee leaves, would one train a new CNN? This would be computationally expensive and take a lot of time to do each time.  \n",
    "\n",
    "Instead, we will try to learn a similarity function, `d(img1, img2)`. This function will be small when two images are of the same person and will be large when the images are of different people. We can have decision rules as follows:\n",
    "\n",
    "if $d(img1, img2) \\leq \\tau$ then predict \"same\", otherwise different.\n",
    "\n",
    "In this way, the face verification problem can be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network\n",
    "\n",
    "One way to solve the face verification problem is the siamese network. It is formulated as a CNN whose final layer head is a vector of some reasonably long length - say 128, an encoding of each image presented.\n",
    "\n",
    "Then we want to learn weights so that the similarity function, $D$, between two images is large if they have different people and small if the images are of the same person.\n",
    "\n",
    "The commonly used measure of similarity is squared euclidean distance - it is mathematically very convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss: FaceNet\n",
    "\n",
    "One way to train the similarity function for little data is to use triplet loss.  Here is how it works:\n",
    "\n",
    "Have three examples in a triplet. One will be the anchor image of the baseline  person's face, the other will be a different image of same face (maybe at another day), called the positive. The third will be a somewhat similar but different person's face, the negative example. Then what we want to happen is that\n",
    "\n",
    "$  E = D(A, P) + \\alpha - D(A, N) \\leq 0 $\n",
    "\n",
    "That is, we want the example anchor image and it's positive to have a small dissimilarity and we want the anchor image and the negative to be large, by some margin $\\alpha \\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function for this triplet learning needs to consider this. What is needed is to minimize:\n",
    "\n",
    "$L(A, P, N) = Max(,0)$\n",
    "\n",
    "That is we want to correctly order the triples by a margin and that happens if the inequality holds and the expression E is negative, but beyond that we aren't concerned with how negative it is. This is why the loss function has a floor of 0, using the max function. \n",
    "\n",
    "A lot of these ideas come from a paper by Schroff et al.(2015). They recommend that the triples should be chosen to be hard. That is there should little difference between the images of the positive and negative examples - so that the network has to work hard to find the similarity function.\n",
    "\n",
    "\n",
    "The commercial applications  of face verification are trained using millions or 10s of millions of images. Some of these companies share the weights and their network architectures online. For example, there is one [here](https://github.com/davidsandberg/facenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Verification:  DeepFace\n",
    "\n",
    "Another way to learn from a siamese network and classify is to pass the final encoding through a logistic regression layer. Then the output will be 1 or 0.\n",
    "\n",
    "The logistic layer is trained as the sigmoid transform applied to the component wise difference vector or component wise chi-squared statistics of comparing a pair.\n",
    "\n",
    "The pair is 1 if the two images are of the same person and 0 if they are of different persons. Since triples are used, then the dataset will be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Neural Style Transfer\n",
    "\n",
    "A fun, novel and exciting application of Deep Learning is Neural Style Transfer. This is to take an image - labeled as `content`, and another labeled as `style` and combine them to create a synthetic generated image - that is the content rendered in the style of another image.\n",
    "\n",
    "![](neural_style_transfer.png)\n",
    "\n",
    "This works by doing the following:\n",
    "\n",
    "1. Defining a cost function that is the weighted sum of two costs. \n",
    "    - the cost of Generated Image G compared to Content Image C, $J_{C}(G,C)$\n",
    "    - the cost of  Generated Image G compared to Style Image S, $J_{S}(G, S)$\n",
    "    - $J = \\alpha J^{C}(G,C) + \\beta J^{S}(G, S)$\n",
    "\n",
    "2. Computing the content cost as the difference of the activations at a certain layer. The deeper in the layer we go, the more high level similarity the Generated image will have to the content image.\n",
    "\n",
    "3. Computing the style cost of the gram correlation matrix across later layers of S and G. \n",
    "\n",
    "4. Applying an optimization algorithm (usually gradient descent) to alter G until the weighted sum cost function J is at a local optima.\n",
    "\n",
    "5. The result is G: the content image C remade with style image S.\n",
    "\n",
    "We'll flesh out the details below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Deep CNNs Learning\n",
    "\n",
    "We can run an experiment where we take a CNN - say the VGG-16 network. Then we work through the layers and pick some neurons in each. If we investigate which images patches maximize the value of each neuron we begin to see patterns. The image patches are of the same family and tend to pick out specific kinds of features - such as lines, circles, eyes, keyboard, faces etc. \n",
    "\n",
    "This is what is happening in a CNN, different neurons are learning different classes of features and they tend to be more complex as we go layers deeper into the network. To the point, where we can see collections of humans or animals such as dogs activating certain very deep neurons.\n",
    "\n",
    "This helps give an intuition as to what is happening when we apply neural style transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "We want to generate an image G which is content C done in the style S based on some network N - usually VGG-16 is a popular network. G is an image, but it is also a matrix of parameters. We define a cost function J of G with respect to fixed input images C and S. We then minimize this cost function with respect to G and get as output $G_opt$, which is the generated image. This is sensitive to the intial conditions we set for G, which are randomly set.\n",
    "\n",
    "Concretely the cost function is composed as:\n",
    "\n",
    " $J  = J(G; C, S, N) = \\alpha J_{C}(G,C) + \\beta J_{S}(G, S)$\n",
    "\n",
    "On gradient descent we would update G as:\n",
    "\n",
    "> Start with $G_0$:\n",
    ">\n",
    "> repeat for some $K$ iterations:\n",
    ">\n",
    ">    $G_{n+1} = G_n - J'(G_n)$\n",
    "    \n",
    "The final G is the generated image.\n",
    "\n",
    "We see that the cost function is a weighted sum of two distinct cost functions. We'll break down these functions below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Cost Function\n",
    "\n",
    "\n",
    "To compute the cost content cost function, a few things are needed first. \n",
    "\n",
    "Something that was omitted is that for neural style transfer, we also need a pretrained network N.  \n",
    "\n",
    "In addition we need to decide a layer number [l] to compare activations/neurons at. If the layer is too small then the updates will make the pixels of G similar to the pixels of C - just replicating the image. If we go too deep then the optimized G will just be looking for some high level representations anywhere in the image. In practice, we suggest somewhere that is not too small and not too large.\n",
    "\n",
    "Having chosen these things, then we define the content part of the cost function to be:\n",
    "\n",
    "$J_C(G) = || A^{[l](G)} - A^{[l](G)} ||_F$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Cost Function\n",
    "\n",
    "As before we need to choose a pretrained network.\n",
    "\n",
    "\n",
    "The style cost function is comparing the squared euclidean distance between the correlation between channels of the generated image G and of the style image S at a given layer [l]  - measuring the extent to which feature maps in a channel are similar. This is the same as the distance between the Gram matrix of S and G.\n",
    "\n",
    "$J^{[l]}_S(G) = || Gram(S)^{[l]} - Gram(G)^{[l]}||_F^2$\n",
    "\n",
    "where $Gram(X) = X^T X$\n",
    "\n",
    "Visually pleasing images are generated if we apply this across all layers. This is expressed by the cost function as:\n",
    "\n",
    "$ J_S(G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_S(G) = \\sum_{l} \\lambda^{[l]} || Gram(S)^{[l]} - Gram(G)^{[l]}||_F^2$\n",
    "\n",
    "\n",
    "The hyperparameters $\\lambda^{[l]} $ can be tuned to get best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D and 3D Generalizations\n",
    "\n",
    "\n",
    "1D data can be convolved with filters to create feature maps in more dimensions. So convnets can be applied. This could be with time series data for example.\n",
    "\n",
    "3D data can be volume convolved with filters to create feature maps in more dimensions. So ConvNets can be applied. Examples would be CT scan, movie data or three dimensional coordinate data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
