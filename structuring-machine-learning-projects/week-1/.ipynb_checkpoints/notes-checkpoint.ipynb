{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model Performance\n",
    "\n",
    "Why is having a disciplined machine learning strategy important? Suppose we train a computer algorithm for a machine learning task. Its performance is not acceptable - we need to improve. There are many changes we can try making to the system, including:\n",
    "\n",
    "* Collect more data\n",
    "\n",
    "* Collect more diverse training data (balance)\n",
    "\n",
    "* Change convergence criteria \n",
    "    - train for more iterations (if using iterative approach)\n",
    "    - train until change is within epsilon\n",
    " \n",
    "* Try different initial parameters\n",
    "\n",
    "* Try different algorithms\n",
    "\n",
    "* Try bigger network (more parameters/complexity)\n",
    "\n",
    "* Try smaller network (more parameters/complexity)\n",
    "\n",
    "* Try dropout\n",
    "\n",
    "* Add regularization\n",
    "\n",
    "* Change Network Architecture\n",
    "    - number of activation units in different layers\n",
    "    - number of hidden layers\n",
    "    - different activation function\n",
    "\n",
    "* Data Augmentation\n",
    "    - artificial data\n",
    "    - similar data\n",
    "    \n",
    "* Bootstrapping\n",
    "\n",
    "* Bagging\n",
    " \n",
    "* Boosting \n",
    " \n",
    "* Early Stopping\n",
    "\n",
    "Changing these controls essentially produces a new solution each time. We are then trying to select the best solution by altering the settings for how it is trained on which kind of data.\n",
    "\n",
    "Almost all of these are good ideas - having different effects depending on the reason underlying unacceptable performance. However, without a strategy we could go in circles trying one thing and then another - possibly wasting time and resources and not getting optimal results. \n",
    "\n",
    "A principled way of diagnosing problems and applying remedies leads to better performance which can be reasoned about much sooner.\n",
    "\n",
    "The difference might be seen as akin to a well trained doctor having good diagnostic tools vs someone with a lot of experience seeing various kinds of illnesses but not having a logical and sound framework to improve with.\n",
    "\n",
    "An important part of applying the right changes is doing them with a sense of fairness - that is that if we change one aspect of the algorithm or data then it should have just one measurable effect on the performance - ideally something that we can reason about.\n",
    "\n",
    "## Orthogonalization\n",
    "\n",
    "In many problems it is convenient to find a set of aspects or system of controls that lets us change one and not change the other controls, only the quantity (for example performance metric) that we are interested in. \n",
    "\n",
    "![](orthogonal_vectors.gif)\n",
    "\n",
    "One example is a co-ordinate system. If we have a set of orthogonal basis vectors then changing the size of one them does not change the others - the basis vectors are perpendicular to each other. If we worked in a non-orthogonal set of basis vectors then change the size of one vector impacts the co-ordinates of the others as well as changing distance.\n",
    "\n",
    "![](non_orthogonal_vectors.png)\n",
    "\n",
    "Another is the controls of a vehicle. Speed and direction are in separate controls (steering wheel and pedals). Changing the speed, influences the distance traveled, but it does not change the direction. Direction also influences the distance, but doesn't affect speed. It is conceivable to create controls that change both direction and speed at the same time, making it much harder to understand and achieve the desired action.\n",
    "\n",
    "![car navigation example](steering_wheel_accelerator_brake.jpg)\n",
    "\n",
    "Another example is an old CRT TV. These televisions had controls or knobs that change height, width, vertical center and horizontal center. There are 4 controls so that each one impacts just one axis. These were carefully set. If instead we had controls that moved both the horizontal and vertical center in one knob - it would be a lot more work.\n",
    "\n",
    "![Old TV](crt_controls.jpg)\n",
    "\n",
    "Similarly, with respect to machine learning performance, we want to have controls that influence performance but not the other controls that we may want to change. Some controls such as \"early stopping\" are not orthogonal to the others and so we don't consider them. We also suggest that one control at a time is changed to see its effect on performance.\n",
    "\n",
    "We will spend the rest of the course understanding how we can identify the reasons for the poor performance and what changes can be expected from change certain controls.\n",
    "\n",
    "## Single Number Evaluation Metric\n",
    "\n",
    "In some cases it might be natural that our performance measure is a vector of numbers. In that case we need to choose one or create one from the vector. It becomes difficult if not impossible to order between a collection of vectors if some go up and others down as we change a control.\n",
    "\n",
    "If we need to optimize a vector of metrics the best suggestion is to compromise and approximate this with a function that takes the vector of metrics as input and outputs a scalar. It can become too difficult to select which control setting to choose otherwise.\n",
    "\n",
    "![single number metric](single_metric_f1.png)\n",
    "\n",
    "One example is wanting a binary classifier to have high precision and high recall. For this, the recommended strategy is to combine recall and precision into their harmonic sum - which is also called the micro-f1 score. Then this f1 score should be improved by changing the controls.\n",
    "\n",
    "For the example above we can order by the f1 score and choose the logistic classifier as the best of them. \n",
    "\n",
    "If we just tried comparing precision and recall, we could only discount the SVM and decision tree as it has strictly worse recall and precision compared to the logistic classifer and random forest respectively. However the other classifiers are dominant in one or other of recall and precision so we don't have a preference relation over them. \n",
    "\n",
    "By having a single metric over a vector we effectively create a utility function that values the different vectors by utility.\n",
    "\n",
    "## Satisficing And Optimizing Metrics\n",
    "\n",
    "Sometimes we might not have a good utility function over the metrics we monitor for the problem in mind.\n",
    "\n",
    "### One Optimizing Metric And Many Satisficing Metrics\n",
    "\n",
    "If we have $N$ metrics on a solution family and no natural utility function over them, we might still have conditions on the metric vector, to help us choose between them.\n",
    "\n",
    "Often we can set one metric as that to be optimized and the others as having some constraint to be satisfied. A good solution is then to put constraints to be satisfied on $N-1$ metrics and retain one to be optimized.\n",
    "\n",
    "By doing this, we can focus on a single number metric, while still chasing desirable features.\n",
    "\n",
    "### Example: Accuracy and Runtime\n",
    "\n",
    "For some time-critical applications (such as web or mobile apps) we can only push to development an application that has a fast enough runtime to be commercially relevant. Although we might get state of the art performance it might have an unacceptable runtime. (This is akin to having the offer of amazing meals four hours after sitting down in a restaurant.)\n",
    "\n",
    "In this case we could threshold our maximum runtime, optimizing accuracy (performance) that meets this satisficing constraint. Any time a solution fails this constraint on the runtime metric we reject it instantly and focus on those that will be at or below the runtime.\n",
    "\n",
    "If we have a family of solutions, then we just filter out those that don't make the threshold cut and then choose the one with the optimal accuracy (optimizing performance) metric.\n",
    "\n",
    "### Example: Wake Word\n",
    "\n",
    "Here we give another example, wake words for devices such as Amazon's Alexa, Google's Home, Apple's Siri and Baidu's Razor. A number of metrics are needed for this problem:\n",
    "\n",
    "* fast runtime\n",
    "* high accuracy\n",
    "* low false positives\n",
    "\n",
    "In industry, fast runtime is not a concern for most models, the hardware and network speeds are fast enough to accommodate most current state of the art algorithm at commercially relevant speeds. Instead, we need to keep the false positives to a reasonable level, say 1 every 24 hours - the satisficing constraint on the metric. Then we need to optimize accuracy while maintaining this constraint.\n",
    "\n",
    "## Train/Dev/Test Distributions\n",
    "\n",
    "Assuming we have enough data - which is often the case in the age of big data then our splitting should be random. We also assume that the data is fairly balanced for classification tasks (the proportions of classes is not excessively different), otherwise we may need over or undersampling.\n",
    "\n",
    "\n",
    "### Selecting Training, Dev And Test Sets \n",
    "\n",
    "Splitting the data into training, dev and test sets is needed because the hyperparameters can't be cleanly optimized before learning the parameters, and a fair assessment of a solution's performance needs to done on unseen data.\n",
    "\n",
    "* training set is used to learn the parameters for some fixed hyperparameters.\n",
    "\n",
    "* dev (also called cross-validation, validation or hold-out) set is used to tune the hyperparameters so that the best parameters can be learned.\n",
    "\n",
    "* test set is used to get an unbiased estimate of the performance of the solution on future data.\n",
    "\n",
    "### Learning vs Tuning\n",
    "\n",
    "On the training set we *optimize* the parameters wrt to some performance measure (cost function) - usually via an iterative procedure such as gradient descent. \n",
    "\n",
    "On the dev set, we *tune* the hyperparameters usually using one of:\n",
    "\n",
    "* random search\n",
    "\n",
    "* grid search\n",
    "\n",
    "* bayesian hyper optimization\n",
    "\n",
    "Dev sets are needed because the hyperparameters can be too difficult (mathematically intractable and/or too computationally expensive) to learn in the iterative optimization process. Using dev sets is akin to performing an empirical Bayes procedure where we learn the best priors using the data itself. \n",
    "\n",
    "The effect tends to be to smooth out the stickiness the parameters of a solution to the features that were realized. The estimates (such as class probabilities or  statistical error  and statistical bias parameters will be flatter and less opinionated, less confident in their predictions). Dev sets are used to regularize the data, their effect is to \n",
    "\n",
    "Test sets are needed because if we try to estimate the expected performance using our training or dev sets, then we will overestimate since we have optimized and tuned over this seen data. No fitting takes place using the test set - it is only to anticipate performance.\n",
    "\n",
    "### Importance Of Same Distribution In Splits\n",
    "\n",
    "In order to tune hyperparameters and get an unbiased estimate of the likely performance on future data, we need to split our dataset into training, dev and test sets. This needs to be done randomly so that the underlying distribution of the splits is the same. In this way our performance measure will be learning and improving for the same type of data.\n",
    "\n",
    "Usually the best strategy is to sample at random - although stratified sampling may be justified in certain instances.\n",
    "\n",
    "### Example: International Application Feature\n",
    "\n",
    "\n",
    "\n",
    "### Example: Loan Applications\n",
    "\n",
    "## Proportions Of Train/Dev/Test Sets\n",
    "\n",
    "## When To Change Dev/Test Sets\n",
    "\n",
    "## Why Human Level Performance\n",
    "\n",
    "## Avoidable Bias\n",
    "\n",
    "## Understanding Human Level Performance\n",
    "\n",
    "## Surpassing Human Level Performance\n",
    "\n",
    "## Improving Model Performance\n",
    "\n",
    "## Carrying Out Error Analysis\n",
    "\n",
    "## Cleaning Up Incorrectly Labeled Data\n",
    "\n",
    "## Build First System Quickly - Then Iterate\n",
    "\n",
    "## Training And Testing On Different Distributions\n",
    "\n",
    "## Bias And Variance With Mismatched Data\n",
    "\n",
    "## Addressing Data Mismatch\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "## Multitask Learning\n",
    "\n",
    "## What is End-To-End Deep Learning\n",
    "\n",
    "## Whether To Use End-To-End Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
