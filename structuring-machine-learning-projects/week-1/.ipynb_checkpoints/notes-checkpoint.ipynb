{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model Performance\n",
    "\n",
    "Why is having a disciplined machine learning strategy important? Suppose we train a computer algorithm for a machine learning task. Its performance is not acceptable - we need to improve. There are many changes we can try making to the system, including:\n",
    "\n",
    "* Collect more data\n",
    "\n",
    "* Collect more diverse training data (balance)\n",
    "\n",
    "* Change convergence criteria \n",
    "    - train for more iterations (if using iterative approach)\n",
    "    - train until change is within epsilon\n",
    " \n",
    "* Try different initial parameters\n",
    "\n",
    "* Try different algorithms\n",
    "\n",
    "* Try bigger network (more parameters/complexity)\n",
    "\n",
    "* Try smaller network (more parameters/complexity)\n",
    "\n",
    "* Try dropout\n",
    "\n",
    "* Add regularization\n",
    "\n",
    "* Change Network Architecture\n",
    "    - number of activation units in different layers\n",
    "    - number of hidden layers\n",
    "    - different activation function\n",
    "\n",
    "* Data Augmentation\n",
    "    - artificial data\n",
    "    - similar data\n",
    "    \n",
    "* Bootstrapping\n",
    "\n",
    "* Bagging\n",
    " \n",
    "* Boosting \n",
    " \n",
    "* Early Stopping\n",
    "\n",
    "Changing these controls essentially produces a new solution each time. We are then trying to select the best solution by altering the settings for how it is trained on which kind of data.\n",
    "\n",
    "Almost all of these are good ideas - having different effects depending on the reason underlying unacceptable performance. However, without a strategy we could go in circles trying one thing and then another - possibly wasting time and resources and not getting optimal results. \n",
    "\n",
    "A principled way of diagnosing problems and applying remedies leads to better performance which can be reasoned about much sooner.\n",
    "\n",
    "The difference might be seen as akin to a well trained doctor having good diagnostic tools vs someone with a lot of experience seeing various kinds of illnesses but not having a logical and sound framework to improve with.\n",
    "\n",
    "An important part of applying the right changes is doing them with a sense of fairness - that is that if we change one aspect of the algorithm or data then it should have just one measurable effect on the performance - ideally something that we can reason about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonalization\n",
    "\n",
    "In many problems it is convenient to find a set of aspects or system of controls that lets us change one and not change the other controls, only the quantity (for example performance metric) that we are interested in. \n",
    "\n",
    "![](orthogonal_vectors.gif)\n",
    "\n",
    "One example is a co-ordinate system. If we have a set of orthogonal basis vectors then changing the size of one them does not change the others - the basis vectors are perpendicular to each other. If we worked in a non-orthogonal set of basis vectors then change the size of one vector impacts the co-ordinates of the others as well as changing distance.\n",
    "\n",
    "![](non_orthogonal_vectors.png)\n",
    "\n",
    "Another is the controls of a vehicle. Speed and direction are in separate controls (steering wheel and pedals). Changing the speed, influences the distance traveled, but it does not change the direction. Direction also influences the distance, but doesn't affect speed. It is conceivable to create controls that change both direction and speed at the same time, making it much harder to understand and achieve the desired action.\n",
    "\n",
    "![car navigation example](steering_wheel_accelerator_brake.jpg)\n",
    "\n",
    "Another example is an old CRT TV. These televisions had controls or knobs that change height, width, vertical center and horizontal center. There are 4 controls so that each one impacts just one axis. These were carefully set. If instead we had controls that moved both the horizontal and vertical center in one knob - it would be a lot more work.\n",
    "\n",
    "![Old TV](crt_controls.jpg)\n",
    "\n",
    "Similarly, with respect to machine learning performance, we want to have controls that influence performance but not the other controls that we may want to change. Some controls such as \"early stopping\" are not orthogonal to the others and so we don't consider them. We also suggest that one control at a time is changed to see its effect on performance.\n",
    "\n",
    "We will spend the rest of the course understanding how we can identify the reasons for the poor performance and what changes can be expected from change certain controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Number Evaluation Metric\n",
    "\n",
    "In some cases it might be natural that our performance measure is a vector of numbers. In that case we need to choose one or create one from the vector. It becomes difficult if not impossible to order between a collection of vectors if some go up and others down as we change a control.\n",
    "\n",
    "If we need to optimize a vector of metrics the best suggestion is to compromise and approximate this with a function that takes the vector of metrics as input and outputs a scalar. It can become too difficult to select which control setting to choose otherwise.\n",
    "\n",
    "![single number metric](single_metric_f1.png)\n",
    "\n",
    "One example is wanting a binary classifier to have high precision and high recall. For this, the recommended strategy is to combine recall and precision into their harmonic sum - which is also called the micro-f1 score. Then this f1 score should be improved by changing the controls.\n",
    "\n",
    "For the example above we can order by the f1 score and choose the logistic classifier as the best of them. \n",
    "\n",
    "If we just tried comparing precision and recall, we could only discount the SVM and decision tree as it has strictly worse recall and precision compared to the logistic classifer and random forest respectively. However the other classifiers are dominant in one or other of recall and precision so we don't have a preference relation over them. \n",
    "\n",
    "By having a single metric over a vector we effectively create a utility function that values the different vectors by utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisficing And Optimizing Metrics\n",
    "\n",
    "Sometimes we might not have a good utility function over the metrics we monitor for the problem in mind.\n",
    "\n",
    "### One Optimizing Metric And Many Satisficing Metrics\n",
    "\n",
    "If we have $N$ metrics on a solution family and no natural utility function over them, we might still have conditions on the metric vector, to help us choose between them.\n",
    "\n",
    "Often we can set one metric as that to be optimized and the others as having some constraint to be satisfied. A good solution is then to put constraints to be satisfied on $N-1$ metrics and retain one to be optimized.\n",
    "\n",
    "By doing this, we can focus on a single number metric, while still chasing desirable features.\n",
    "\n",
    "### Example: Accuracy and Runtime\n",
    "\n",
    "For some time-critical applications (such as web or mobile apps) we can only push to development an application that has a fast enough runtime to be commercially relevant. Although we might get state of the art performance it might have an unacceptable runtime. (This is akin to having the offer of amazing meals four hours after sitting down in a restaurant.)\n",
    "\n",
    "In this case we could threshold our maximum runtime, optimizing accuracy (performance) that meets this satisficing constraint. Any time a solution fails this constraint on the runtime metric we reject it instantly and focus on those that will be at or below the runtime.\n",
    "\n",
    "If we have a family of solutions, then we just filter out those that don't make the threshold cut and then choose the one with the optimal accuracy (optimizing performance) metric.\n",
    "\n",
    "### Example: Wake Word\n",
    "\n",
    "Here we give another example, wake words for devices such as Amazon's Alexa, Google's Home, Apple's Siri and Baidu's Razor. A number of metrics are needed for this problem:\n",
    "\n",
    "* fast runtime\n",
    "* high accuracy\n",
    "* low false positives\n",
    "\n",
    "In industry, fast runtime is not a concern for most models, the hardware and network speeds are fast enough to accommodate most current state of the art algorithm at commercially relevant speeds. Instead, we need to keep the false positives to a reasonable level, say 1 every 24 hours - the satisficing constraint on the metric. Then we need to optimize accuracy while maintaining this constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test Distributions\n",
    "\n",
    "Assuming we have enough data - which is often the case in the age of big data then our splitting should be random. We also assume that the data is fairly balanced for classification tasks (the proportions of classes is not excessively different), otherwise we may need over or undersampling.\n",
    "\n",
    "\n",
    "### Selecting Training, Dev And Test Sets \n",
    "\n",
    "Splitting the data into training, dev and test sets is needed because the hyperparameters can't be cleanly optimized before learning the parameters, and a fair assessment of a solution's performance needs to done on unseen data.\n",
    "\n",
    "* training set is used to learn the parameters for some fixed hyperparameters.\n",
    "\n",
    "* dev (also called cross-validation, validation or hold-out) set is used to tune the hyperparameters so that the best parameters can be learned.\n",
    "\n",
    "* test set is used to get an unbiased estimate of the performance of the solution on future data.\n",
    "\n",
    "### Learning vs Tuning\n",
    "\n",
    "On the training set we *optimize* the parameters wrt to some performance measure (cost function) - usually via an iterative procedure such as gradient descent. \n",
    "\n",
    "On the dev set, we *tune* the hyperparameters usually using one of:\n",
    "\n",
    "* random search\n",
    "\n",
    "* grid search\n",
    "\n",
    "* bayesian hyper optimization\n",
    "\n",
    "Dev sets are needed because the hyperparameters can be too difficult (mathematically intractable and/or too computationally expensive) to learn in the iterative optimization process. Using dev sets is akin to performing an empirical Bayes procedure where we learn the best priors using the data itself. \n",
    "\n",
    "The effect tends to be to smooth out the stickiness the parameters of a solution to the features that were realized. The estimates (such as class probabilities or  statistical error  and statistical bias parameters will be flatter and less opinionated, less confident in their predictions). Dev sets are used to regularize the data, their effect is to \n",
    "\n",
    "Test sets are needed because if we try to estimate the expected performance using our training or dev sets, then we will overestimate since we have optimized and tuned over this seen data. No fitting takes place using the test set - it is only to anticipate performance.\n",
    "\n",
    "### Importance Of Same Distribution In Splits\n",
    "\n",
    "In order to tune hyperparameters and get an unbiased estimate of the likely performance on future data, we need to split our dataset into training, dev and test sets. This needs to be done randomly so that the underlying distribution of the splits is the same. In this way our performance measure will be learning and improving for the same type of data.\n",
    "\n",
    "Usually the best strategy is to sample at random - although stratified sampling may be justified in certain instances.\n",
    "\n",
    "### Example: International Smartphone App ML Feature\n",
    "\n",
    "Imagine we have an smartphone app which has some ML feature (such as recognizing cats from images!), we may even be presented the data into groupings by country (from ingestion via a SQL/WebService API). Say our data is easily available as being from one of :\n",
    "\n",
    "* US\n",
    "* Europe\n",
    "* UK\n",
    "* China\n",
    "* India\n",
    "* SE Asia\n",
    "* LatAm\n",
    "* Canada\n",
    "* Russia\n",
    "\n",
    "We might be tempted to take these splits as natural and put the first three in training, the second three in dev and the final three in test set.\n",
    "\n",
    "However, this is bad policy. If we do so, then we can expect that the distributions of the data are very different for the training and dev sets. For example (more light and worse cameras in smartphones used in Asia).\n",
    "\n",
    "Instead we should randomize the train/dev/test to get good performance. Alternatively, we might build a solution for each country if we truly believe the distributions are quite different for each country.\n",
    "\n",
    "### Example: Loan Applications\n",
    "\n",
    "Suppose we make train and dev sets to calculate the probability of default only for people who live in medium income zipcodes. We go with a fully connected feedforward neural network and use the nll cost function and are careful to check our data is fairly balanced. We regularize with the dev set. \n",
    "\n",
    "Now if a manager brings us a test set from low income zipcodes our model will likely underestimate the probability of default (assuming that low income zipcodes are associated with higher defaults than medium income zipcodes) and our performance measure (the nll cost function) will do badly.\n",
    "\n",
    "The problem is that the test set has a different distribution than the train and dev set. It would have been better to train using income from low income zipcodes in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportions Of Train/Dev/Test Sets\n",
    "\n",
    "Before big data (with smaller data sets and less compute) it was normal to have only train/test sets of:\n",
    "\n",
    "* Train: 70% \n",
    "* Test: 30%\n",
    "\n",
    "In the past hyperparameters were not always tuned.\n",
    "\n",
    "It was also common to sometimes skip the test estimate and split with train and dev and then ship the solution:\n",
    "\n",
    "* Train: 70% \n",
    "* Dev: 30%\n",
    "\n",
    "At the time of my PhD, I used:\n",
    "\n",
    "* Train: 70% \n",
    "* Dev: 15%\n",
    "* Test: 15%\n",
    "\n",
    "However, now with much larger data sets - the era of big data, it's become common to give most of the data to data hungry training algorithms (with many parameters) and leave much smaller amounts of data for dev for a smaller number of hyperparameters and even less is needed for the test set to estimate expected performance. \n",
    "\n",
    "Nowadays splits as follows are normal:\n",
    "\n",
    "* Train: 99% \n",
    "* Dev: 0.9%\n",
    "* Test: 0.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When To Change Dev/Test Sets\n",
    "\n",
    "### Metric Error\n",
    "There are cases where the performance metric is not properly rank ordering solutions, perhaps because it fails on a satisficing constraint that was not made explicit and assumed obvious.\n",
    "\n",
    "\n",
    "In such a case introducing a penalty for the unexpected constraint should be done. The performance measure should be penalized for failing the constraint or as before filters can be placed when ordering between solutions.\n",
    "\n",
    "#### Example: Pornographic Images \n",
    "\n",
    "Imagine we have cat image app. We show different images of cats from user uploads to other users. We trained a few classifiers, decided that accuracy is the performance metrric to target. One of the classifiers has a very high accuracy, 97%, let's call this classifier A. Another classifier B, has lower accuracy at 95%. It would seem that classifier A should be used.\n",
    "\n",
    "However in our trials we find that nefarious users can upload pornographic images and classifier A, our preferred classifier will let them through. By contrast classifier B never lets the pornographic images through.\n",
    "\n",
    "So we want classifier B to be used in production, even though our evaluation metric suggests classifier A. \n",
    "\n",
    "There are two solutions to problems such as this.\n",
    "\n",
    "* Recognize that a new satisificing constraint has appeared - one that had never been needed before. So introduce the constraint and classifier A is no longer a candidate. We would choose B.\n",
    "\n",
    "* Alternatively, we can introduce a penalty on our performance measure so that examples with pornographic images classified as cats are heavily penalized.\n",
    "\n",
    "Of these I prefer the first, as this doesn't affect the accuracy measure, since this is a natural metric. In fact I would prefer the satisficing constraint and to use the nll as a performance metric - there are stronger theoretical reasons to prefer the nll over accuracy.\n",
    "\n",
    "### Orthogonalize Metric Choice From Improving Solution\n",
    "\n",
    "Separate out:\n",
    "\n",
    "- Defining Performance Evaluation Metric\n",
    "\n",
    "from \n",
    "\n",
    "- Improving Performance via applying strategies to various controls\n",
    "\n",
    "The two things should be handled separately as they can be confused if one changes the metric part way through improving it. If we have a good justification for our metric, then improving it should just follow the strategies we outlined.\n",
    "\n",
    "\n",
    "\n",
    "### Different Distributions for Dev and Test Sets\n",
    "\n",
    "If distributions are different then change dev and test sets so that their distributions are the same. The difference may occur if despite our best efforts training and/or dev sets have different distributions from the test set - which is usually a proxy for the production environment.\n",
    "\n",
    "#### Example: Cat Images \n",
    "\n",
    "Image we have trained our cat images classifier with images from the web. We take a dev set from the same distribution and feel happy about our performance, maybe we get a test set from the web images and feel very happy about our expected performance. \n",
    "\n",
    "However, as we push to production, we see significantly worse performance - the reason that the camera and photo conditions on a smartphone are not close to the images crawled from the web - those have professional lighting and high quality cameras.\n",
    "\n",
    "\n",
    "In such a case we should change our train and dev set to match the distribution of the test set - the production environment. \n",
    "\n",
    "Of course this might not be possible if say there are not enough smartphone images to train a deep network on. In such a case we suggest strategies to modify web data to be a better proxy for smartphone data, or else synthesize more data using the smartphone images and generate more artificial images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Human Level Performance\n",
    "\n",
    "### Human Level Error\n",
    "\n",
    "Humans are very good at dealing with structured data - we have thousands of years of evolution to deal with such data.\n",
    "\n",
    "### Bayes Optimal Error\n",
    "\n",
    "Impossible to perform better than this limit, which is the natural randomness in the data generating process. For classification:\n",
    "\n",
    "$$\n",
    "p =  1- \\textstyle  \\sum_{C_{i} \\neq C_\\text{max,x}} \\int\\limits_{x\\in H_{i}} P(C_{i}|x)p(x)\\, dx\n",
    "$$\n",
    "Usually Bayes Error is referencing the Bayes Error Rate for classifiers - which is the probability that the classifier's modal class is not the one that occurs. This is non-zero if the classifier is not deterministic.\n",
    "\n",
    "There is an analogy for regression, which is incorporate all relevant features, select the correct model and estimate parameters so well that the only error is due to the randomness in the data generating process - however this is not usually referenced.\n",
    "\n",
    "### Why does performance often plateau after surpassing  Human Level\n",
    "\n",
    "-  The human level error can be very close to the Bayes error\n",
    "\n",
    "- We have tools to improve performance as long as performance is below the human threshold making:\n",
    "    - use human labelling to get more data\n",
    "    - manual error analysis to gain insight\n",
    "\n",
    "\n",
    "We can use this human input of more data and error analysis to help better understand bias and variance problems with a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoidable Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Human Level Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surpassing Human Level Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
